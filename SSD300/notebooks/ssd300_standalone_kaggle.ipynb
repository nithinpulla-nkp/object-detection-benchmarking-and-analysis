{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "# SSD300 Complete Standalone Pipeline for Kaggle\n",
        "\n",
        "This notebook contains everything needed to train and evaluate SSD300 on Pascal VOC 2012 dataset.\n",
        "It's designed to run standalone in Kaggle without external dependencies.\n",
        "\n",
        "## Features:\n",
        "- Complete VOC to COCO conversion\n",
        "- SSD300 training with enhanced logging\n",
        "- Comprehensive evaluation with COCO metrics\n",
        "- Visual comparisons of ground truth vs predictions\n",
        "- Model checkpointing and visualization\n",
        "- Comparison images testing from comparison_images.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import logging\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ML/DL libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# COCO evaluation\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# Configure matplotlib and seaborn\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## 2. Configuration and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION - MODIFY THESE PATHS FOR YOUR SETUP =====\n",
        "\n",
        "# For Kaggle, update these paths to match your Kaggle setup\n",
        "CONFIG = {\n",
        "    # Dataset paths (modify for Kaggle)\n",
        "    'voc_root': '/kaggle/input/voc2012/VOCdevkit/VOC2012',  # Kaggle input path\n",
        "    'output_dir': '/kaggle/working/ssd300_outputs',  # Kaggle working directory\n",
        "    \n",
        "    # Training parameters\n",
        "    'batch_size': 4,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 1e-4,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_workers': 2,\n",
        "    \n",
        "    # Evaluation parameters\n",
        "    'conf_threshold': 0.3,\n",
        "    'nms_threshold': 0.45,\n",
        "    \n",
        "    # Experiment settings\n",
        "    'experiment_name': f'ssd300_voc_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
        "    'save_checkpoints': True,\n",
        "    'checkpoint_every': 1,  # Save every N epochs\n",
        "    'save_visualizations': True,\n",
        "    'run_comparison_images': True\n",
        "}\n",
        "\n",
        "# VOC Classes\n",
        "VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "\n",
        "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(VOC_CLASSES)}\n",
        "IDX_TO_CLASS = {idx: cls for idx, cls in enumerate(VOC_CLASSES)}\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/logs\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/predictions\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/visualizations\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/data\", exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüìÅ Output directory created: {CONFIG['output_dir']}\")\n",
        "print(f\"üéØ Experiment name: {CONFIG['experiment_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## 3. Enhanced Logging Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup comprehensive logging\n",
        "log_file = f\"{CONFIG['output_dir']}/logs/{CONFIG['experiment_name']}.log\"\n",
        "\n",
        "# Create custom logger\n",
        "logger = logging.getLogger('SSD300_Pipeline')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Clear existing handlers\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "# File handler\n",
        "file_handler = logging.FileHandler(log_file)\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Console handler\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Training metrics tracking\n",
        "training_metrics = {\n",
        "    'start_time': None,\n",
        "    'end_time': None,\n",
        "    'epoch_times': [],\n",
        "    'train_losses': [],\n",
        "    'learning_rates': [],\n",
        "    'best_map': 0.0,\n",
        "    'final_metrics': {},\n",
        "    'total_parameters': 0,\n",
        "    'model_size_mb': 0\n",
        "}\n",
        "\n",
        "logger.info(f\"Starting SSD300 experiment: {CONFIG['experiment_name']}\")\n",
        "logger.info(f\"Configuration: {CONFIG}\")\n",
        "logger.info(f\"Log file: {log_file}\")\n",
        "\n",
        "print(f\"‚úÖ Enhanced logging setup complete\")\n",
        "print(f\"üìù Log file: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## 4. VOC to COCO Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_voc_to_coco(voc_root, output_file, image_set='trainval'):\n",
        "    \"\"\"\n",
        "    Convert Pascal VOC dataset to COCO format\n",
        "    \"\"\"\n",
        "    logger.info(f\"Converting VOC to COCO format for {image_set} set...\")\n",
        "    \n",
        "    voc_path = Path(voc_root)\n",
        "    \n",
        "    # Initialize COCO format structure\n",
        "    coco_format = {\n",
        "        \"info\": {\n",
        "            \"description\": \"Pascal VOC 2012 in COCO format\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2012,\n",
        "            \"contributor\": \"SSD300 Pipeline\",\n",
        "            \"date_created\": datetime.now().isoformat()\n",
        "        },\n",
        "        \"licenses\": [{\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Pascal VOC License\",\n",
        "            \"url\": \"http://host.robots.ox.ac.uk/pascal/VOC/\"\n",
        "        }],\n",
        "        \"categories\": [],\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "    \n",
        "    # Add categories\n",
        "    for idx, class_name in enumerate(VOC_CLASSES):\n",
        "        coco_format[\"categories\"].append({\n",
        "            \"id\": idx + 1,  # COCO categories start from 1\n",
        "            \"name\": class_name,\n",
        "            \"supercategory\": \"object\"\n",
        "        })\n",
        "    \n",
        "    # Read image IDs\n",
        "    image_set_file = voc_path / 'ImageSets' / 'Main' / f'{image_set}.txt'\n",
        "    if not image_set_file.exists():\n",
        "        logger.error(f\"Image set file not found: {image_set_file}\")\n",
        "        return False\n",
        "    \n",
        "    with open(image_set_file, 'r') as f:\n",
        "        image_ids = [line.strip() for line in f.readlines()]\n",
        "    \n",
        "    annotation_id = 1\n",
        "    conversion_stats = {'total_images': 0, 'total_annotations': 0, 'skipped_images': 0}\n",
        "    \n",
        "    logger.info(f\"Processing {len(image_ids)} images...\")\n",
        "    \n",
        "    for idx, image_id in enumerate(image_ids):\n",
        "        if idx % 500 == 0:\n",
        "            logger.info(f\"Processed {idx}/{len(image_ids)} images\")\n",
        "        \n",
        "        # Image file\n",
        "        img_file = voc_path / 'JPEGImages' / f'{image_id}.jpg'\n",
        "        if not img_file.exists():\n",
        "            logger.warning(f\"Image file not found: {img_file}\")\n",
        "            conversion_stats['skipped_images'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Get image dimensions\n",
        "        try:\n",
        "            with Image.open(img_file) as img:\n",
        "                width, height = img.size\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Cannot read image {img_file}: {e}\")\n",
        "            conversion_stats['skipped_images'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Add image info\n",
        "        image_info = {\n",
        "            \"id\": int(image_id) if image_id.isdigit() else hash(image_id) % (10**8),\n",
        "            \"file_name\": f\"{image_id}.jpg\",\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"license\": 1\n",
        "        }\n",
        "        coco_format[\"images\"].append(image_info)\n",
        "        conversion_stats['total_images'] += 1\n",
        "        \n",
        "        # Process annotations\n",
        "        xml_file = voc_path / 'Annotations' / f'{image_id}.xml'\n",
        "        if not xml_file.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            tree = ET.parse(xml_file)\n",
        "            root = tree.getroot()\n",
        "            \n",
        "            for obj in root.findall('object'):\n",
        "                class_name = obj.find('name').text\n",
        "                if class_name not in CLASS_TO_IDX:\n",
        "                    continue\n",
        "                \n",
        "                # Get bounding box\n",
        "                bbox_elem = obj.find('bndbox')\n",
        "                xmin = float(bbox_elem.find('xmin').text)\n",
        "                ymin = float(bbox_elem.find('ymin').text)\n",
        "                xmax = float(bbox_elem.find('xmax').text)\n",
        "                ymax = float(bbox_elem.find('ymax').text)\n",
        "                \n",
        "                # Convert to COCO format (x, y, width, height)\n",
        "                bbox_width = xmax - xmin\n",
        "                bbox_height = ymax - ymin\n",
        "                area = bbox_width * bbox_height\n",
        "                \n",
        "                # Add annotation\n",
        "                annotation = {\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_info[\"id\"],\n",
        "                    \"category_id\": CLASS_TO_IDX[class_name] + 1,  # COCO categories start from 1\n",
        "                    \"bbox\": [xmin, ymin, bbox_width, bbox_height],\n",
        "                    \"area\": area,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                coco_format[\"annotations\"].append(annotation)\n",
        "                annotation_id += 1\n",
        "                conversion_stats['total_annotations'] += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing annotations for {image_id}: {e}\")\n",
        "    \n",
        "    # Save COCO format JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_format, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"VOC to COCO conversion completed!\")\n",
        "    logger.info(f\"Statistics: {conversion_stats}\")\n",
        "    logger.info(f\"COCO file saved to: {output_file}\")\n",
        "    \n",
        "    return conversion_stats\n",
        "\n",
        "print(\"‚úÖ VOC to COCO conversion functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "## 5. Dataset Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "class COCODetectionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCO format dataset for object detection\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, coco_json_path, image_dir, transforms=None):\n",
        "        self.coco = COCO(coco_json_path)\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.transforms = transforms\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        \n",
        "        logger.info(f\"Dataset initialized with {len(self.ids)} images\")\n",
        "        logger.info(f\"Image directory: {self.image_dir}\")\n",
        "        logger.info(f\"COCO file: {coco_json_path}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        img_info = self.coco.imgs[img_id]\n",
        "        \n",
        "        # Load image\n",
        "        img_path = self.image_dir / img_info['file_name']\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Get annotations\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        \n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            # Convert to (xmin, ymin, xmax, ymax)\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            # Convert category_id back to 0-based indexing\n",
        "            labels.append(ann['category_id'] - 1)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        \n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([img_id], dtype=torch.int64)\n",
        "        }\n",
        "        \n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "# Custom collate function for object detection\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for object detection datasets\n",
        "    \"\"\"\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    return images, list(targets)\n",
        "\n",
        "print(\"‚úÖ Dataset classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "## 6. Model Setup and Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ssd300_model(num_classes=20, pretrained=True):\n",
        "    \"\"\"\n",
        "    Create SSD300 model with VGG16 backbone\n",
        "    \"\"\"\n",
        "    logger.info(f\"Creating SSD300 model with {num_classes} classes (pretrained={pretrained})\")\n",
        "    \n",
        "    # Load pretrained SSD300 model\n",
        "    model = ssd300_vgg16(pretrained=pretrained, progress=True)\n",
        "    \n",
        "    # Replace the classification head for VOC classes\n",
        "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
        "    num_anchors = [4, 6, 6, 6, 4, 4]\n",
        "    \n",
        "    model.head.classification_head = SSDClassificationHead(\n",
        "        in_channels, num_anchors, num_classes + 1  # +1 for background\n",
        "    )\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    logger.info(f\"Model created successfully\")\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    training_metrics['total_parameters'] = total_params\n",
        "    \n",
        "    return model\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filepath, is_best=False):\n",
        "    \"\"\"\n",
        "    Save model checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'training_metrics': training_metrics,\n",
        "        'config': CONFIG\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, filepath)\n",
        "    logger.info(f\"Checkpoint saved to {filepath}\")\n",
        "    \n",
        "    if is_best:\n",
        "        best_path = filepath.replace('.pth', '_best.pth')\n",
        "        torch.save(checkpoint, best_path)\n",
        "        logger.info(f\"Best model saved to {best_path}\")\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device, epoch):\n",
        "    \"\"\"\n",
        "    Train model for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    logger.info(f\"Starting epoch {epoch + 1} training...\")\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "        # Move data to device\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
        "                   for k, v in target.items()} for target in targets]\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        \n",
        "        # Calculate total loss\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Backward pass\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        \n",
        "        # Log progress\n",
        "        if batch_idx % 10 == 0 or batch_idx == num_batches - 1:\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            logger.info(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}, \"\n",
        "                       f\"Batch {batch_idx + 1}/{num_batches}, \"\n",
        "                       f\"Loss: {losses.item():.4f}, \"\n",
        "                       f\"Avg Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = total_loss / num_batches\n",
        "    \n",
        "    logger.info(f\"Epoch {epoch + 1} completed in {epoch_time:.1f}s, \"\n",
        "               f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
        "    \n",
        "    training_metrics['epoch_times'].append(epoch_time)\n",
        "    training_metrics['train_losses'].append(avg_epoch_loss)\n",
        "    \n",
        "    return avg_epoch_loss\n",
        "\n",
        "print(\"‚úÖ Model and training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "## 7. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device, coco_gt):\n",
        "    \"\"\"\n",
        "    Evaluate model using COCO metrics\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting model evaluation...\")\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "            if batch_idx % 20 == 0:\n",
        "                logger.info(f\"Evaluation batch {batch_idx + 1}/{len(dataloader)}\")\n",
        "            \n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Get predictions\n",
        "            predictions = model(images)\n",
        "            \n",
        "            # Process each image in the batch\n",
        "            for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
        "                img_id = target['image_id'].item()\n",
        "                \n",
        "                boxes = pred['boxes'].cpu().numpy()\n",
        "                scores = pred['scores'].cpu().numpy()\n",
        "                labels = pred['labels'].cpu().numpy()\n",
        "                \n",
        "                # Filter by confidence threshold\n",
        "                keep = scores >= CONFIG['conf_threshold']\n",
        "                boxes = boxes[keep]\n",
        "                scores = scores[keep]\n",
        "                labels = labels[keep]\n",
        "                \n",
        "                # Convert to COCO format\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    all_predictions.append({\n",
        "                        \"image_id\": img_id,\n",
        "                        \"category_id\": int(label) + 1,  # Convert back to 1-based\n",
        "                        \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "    \n",
        "    if not all_predictions:\n",
        "        logger.warning(\"No predictions generated!\")\n",
        "        return {}\n",
        "    \n",
        "    # Save predictions\n",
        "    pred_file = f\"{CONFIG['output_dir']}/predictions/ssd300_predictions.json\"\n",
        "    with open(pred_file, 'w') as f:\n",
        "        json.dump(all_predictions, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"Generated {len(all_predictions)} predictions\")\n",
        "    logger.info(f\"Predictions saved to {pred_file}\")\n",
        "    \n",
        "    # Evaluate with COCO metrics\n",
        "    try:\n",
        "        coco_pred = coco_gt.loadRes(pred_file)\n",
        "        coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        coco_eval.summarize()\n",
        "        \n",
        "        # Extract metrics\n",
        "        metrics = {\n",
        "            'mAP_0.5_0.95': coco_eval.stats[0],\n",
        "            'mAP_0.5': coco_eval.stats[1],\n",
        "            'mAP_0.75': coco_eval.stats[2],\n",
        "            'mAP_small': coco_eval.stats[3],\n",
        "            'mAP_medium': coco_eval.stats[4],\n",
        "            'mAP_large': coco_eval.stats[5],\n",
        "            'AR_1': coco_eval.stats[6],\n",
        "            'AR_10': coco_eval.stats[7],\n",
        "            'AR_100': coco_eval.stats[8],\n",
        "            'AR_small': coco_eval.stats[9],\n",
        "            'AR_medium': coco_eval.stats[10],\n",
        "            'AR_large': coco_eval.stats[11]\n",
        "        }\n",
        "        \n",
        "        logger.info(\"COCO Evaluation Results:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            logger.info(f\"  {metric_name}: {value:.4f}\")\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"COCO evaluation failed: {e}\")\n",
        "        return {}\n",
        "\n",
        "print(\"‚úÖ Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "## 8. Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_training_progress(training_metrics, save_path=None):\n",
        "    \"\"\"\n",
        "    Create comprehensive training progress visualization\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f'SSD300 Training Progress - {CONFIG[\"experiment_name\"]}', fontsize=16)\n",
        "    \n",
        "    # Training loss\n",
        "    if training_metrics['train_losses']:\n",
        "        axes[0, 0].plot(training_metrics['train_losses'], 'b-', linewidth=2)\n",
        "        axes[0, 0].set_title('Training Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Epoch times\n",
        "    if training_metrics['epoch_times']:\n",
        "        axes[0, 1].plot(training_metrics['epoch_times'], 'g-', linewidth=2)\n",
        "        axes[0, 1].set_title('Training Time per Epoch')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Time (seconds)')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning rates\n",
        "    if training_metrics['learning_rates']:\n",
        "        axes[1, 0].plot(training_metrics['learning_rates'], 'r-', linewidth=2)\n",
        "        axes[1, 0].set_title('Learning Rate Schedule')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Learning Rate')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_yscale('log')\n",
        "    \n",
        "    # Summary statistics\n",
        "    axes[1, 1].axis('off')\n",
        "    summary_text = f\"\"\"\n",
        "Training Summary:\n",
        "‚Ä¢ Total Parameters: {training_metrics['total_parameters']:,}\n",
        "‚Ä¢ Epochs Completed: {len(training_metrics['train_losses'])}\n",
        "‚Ä¢ Final Loss: {training_metrics['train_losses'][-1]:.4f if training_metrics['train_losses'] else 'N/A'}\n",
        "‚Ä¢ Best mAP@0.5: {training_metrics['best_map']:.4f}\n",
        "‚Ä¢ Avg Epoch Time: {np.mean(training_metrics['epoch_times']):.1f}s\n",
        "‚Ä¢ Total Training Time: {sum(training_metrics['epoch_times']):.1f}s\n",
        "\"\"\"\n",
        "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        logger.info(f\"Training progress plot saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions(model, dataset, device, num_images=5, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions on sample images\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Select random images\n",
        "    indices = random.sample(range(len(dataset)), min(num_images, len(dataset)))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, len(indices), figsize=(4 * len(indices), 8))\n",
        "    if len(indices) == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, target = dataset[idx]\n",
        "            \n",
        "            # Get prediction\n",
        "            model_input = image.unsqueeze(0).to(device)\n",
        "            prediction = model(model_input)[0]\n",
        "            \n",
        "            # Convert image back to PIL for visualization\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                # Denormalize if needed\n",
        "                img_array = image.permute(1, 2, 0).numpy()\n",
        "                if img_array.min() < 0:  # Normalized image\n",
        "                    img_array = (img_array * 0.229) + 0.485  # Approximate denormalization\n",
        "                img_array = np.clip(img_array, 0, 1)\n",
        "            else:\n",
        "                img_array = np.array(image) / 255.0\n",
        "            \n",
        "            # Ground truth visualization\n",
        "            axes[0, i].imshow(img_array)\n",
        "            axes[0, i].set_title(f'Ground Truth')\n",
        "            axes[0, i].axis('off')\n",
        "            \n",
        "            # Draw ground truth boxes\n",
        "            if len(target['boxes']) > 0:\n",
        "                for box, label in zip(target['boxes'], target['labels']):\n",
        "                    x1, y1, x2, y2 = box.numpy()\n",
        "                    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                           linewidth=2, edgecolor='green', facecolor='none')\n",
        "                    axes[0, i].add_patch(rect)\n",
        "                    axes[0, i].text(x1, y1-5, VOC_CLASSES[label.item()], \n",
        "                                   color='green', fontsize=8, weight='bold')\n",
        "            \n",
        "            # Prediction visualization\n",
        "            axes[1, i].imshow(img_array)\n",
        "            axes[1, i].set_title(f'Predictions')\n",
        "            axes[1, i].axis('off')\n",
        "            \n",
        "            # Draw prediction boxes\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "            \n",
        "            # Filter by confidence\n",
        "            keep = scores >= CONFIG['conf_threshold']\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            \n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
        "                axes[1, i].add_patch(rect)\n",
        "                axes[1, i].text(x1, y1-5, f'{VOC_CLASSES[label]} ({score:.2f})', \n",
        "                               color='red', fontsize=8, weight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        logger.info(f\"Prediction visualization saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "## 9. Comparison Images Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_comparison_images(model, dataset, device, comparison_file_path=None):\n",
        "    \"\"\"\n",
        "    Test model on comparison images and create side-by-side visualizations\n",
        "    \"\"\"\n",
        "    # Define comparison images if file not provided\n",
        "    if comparison_file_path and os.path.exists(comparison_file_path):\n",
        "        with open(comparison_file_path, 'r') as f:\n",
        "            comparison_data = json.load(f)\n",
        "        comparison_images = comparison_data.get('images', [])\n",
        "    else:\n",
        "        # Define some default comparison images\n",
        "        comparison_images = [\n",
        "            {\"image_id\": \"2008_000002\", \"filename\": \"2008_000002.jpg\", \n",
        "             \"description\": \"Person with horse\", \"expected_objects\": [\"person\", \"horse\"], \"difficulty\": \"medium\"},\n",
        "            {\"image_id\": \"2008_000008\", \"filename\": \"2008_000008.jpg\", \n",
        "             \"description\": \"Multiple cars\", \"expected_objects\": [\"car\"], \"difficulty\": \"easy\"},\n",
        "            {\"image_id\": \"2008_000015\", \"filename\": \"2008_000015.jpg\", \n",
        "             \"description\": \"Birds in scene\", \"expected_objects\": [\"bird\"], \"difficulty\": \"hard\"},\n",
        "            {\"image_id\": \"2008_000019\", \"filename\": \"2008_000019.jpg\", \n",
        "             \"description\": \"Person scene\", \"expected_objects\": [\"person\"], \"difficulty\": \"medium\"},\n",
        "            {\"image_id\": \"2008_000021\", \"filename\": \"2008_000021.jpg\", \n",
        "             \"description\": \"Multiple objects\", \"expected_objects\": [\"person\", \"bicycle\"], \"difficulty\": \"hard\"}\n",
        "        ]\n",
        "    \n",
        "    logger.info(f\"Testing on {len(comparison_images)} comparison images...\")\n",
        "    \n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    # Find comparison images in dataset\n",
        "    dataset_filenames = {}\n",
        "    for idx in range(len(dataset)):\n",
        "        _, target = dataset[idx]\n",
        "        img_id = target['image_id'].item()\n",
        "        img_info = dataset.coco.imgs[img_id]\n",
        "        dataset_filenames[img_info['file_name']] = idx\n",
        "    \n",
        "    comparison_found = []\n",
        "    for img_data in comparison_images:\n",
        "        if img_data['filename'] in dataset_filenames:\n",
        "            comparison_found.append((img_data, dataset_filenames[img_data['filename']]))\n",
        "    \n",
        "    if not comparison_found:\n",
        "        logger.warning(\"No comparison images found in dataset!\")\n",
        "        return []\n",
        "    \n",
        "    logger.info(f\"Found {len(comparison_found)} comparison images in dataset\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(len(comparison_found), 2, figsize=(12, 4 * len(comparison_found)))\n",
        "    if len(comparison_found) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for row, (img_data, idx) in enumerate(comparison_found):\n",
        "            image, target = dataset[idx]\n",
        "            \n",
        "            # Get prediction\n",
        "            model_input = image.unsqueeze(0).to(device)\n",
        "            prediction = model(model_input)[0]\n",
        "            \n",
        "            # Convert image for visualization\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                img_array = image.permute(1, 2, 0).numpy()\n",
        "                if img_array.min() < 0:  # Normalized\n",
        "                    img_array = (img_array * 0.229) + 0.485\n",
        "                img_array = np.clip(img_array, 0, 1)\n",
        "            else:\n",
        "                img_array = np.array(image) / 255.0\n",
        "            \n",
        "            # Ground truth (left side)\n",
        "            axes[row, 0].imshow(img_array)\n",
        "            axes[row, 0].set_title(f'Ground Truth - {img_data[\"description\"]}\\nExpected: {\", \".join(img_data[\"expected_objects\"])}')\n",
        "            axes[row, 0].axis('off')\n",
        "            \n",
        "            # Draw ground truth boxes\n",
        "            gt_objects = []\n",
        "            if len(target['boxes']) > 0:\n",
        "                for box, label in zip(target['boxes'], target['labels']):\n",
        "                    x1, y1, x2, y2 = box.numpy()\n",
        "                    class_name = VOC_CLASSES[label.item()]\n",
        "                    gt_objects.append(class_name)\n",
        "                    \n",
        "                    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                           linewidth=2, edgecolor='green', facecolor='none')\n",
        "                    axes[row, 0].add_patch(rect)\n",
        "                    axes[row, 0].text(x1, y1-5, class_name, \n",
        "                                     color='green', fontsize=10, weight='bold',\n",
        "                                     bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
        "            \n",
        "            # Predictions (right side)\n",
        "            axes[row, 1].imshow(img_array)\n",
        "            axes[row, 1].set_title(f'Predictions - Difficulty: {img_data[\"difficulty\"]}')\n",
        "            axes[row, 1].axis('off')\n",
        "            \n",
        "            # Draw prediction boxes\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "            \n",
        "            # Filter by confidence\n",
        "            keep = scores >= CONFIG['conf_threshold']\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            \n",
        "            pred_objects = []\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "                class_name = VOC_CLASSES[label]\n",
        "                pred_objects.append(class_name)\n",
        "                \n",
        "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
        "                axes[row, 1].add_patch(rect)\n",
        "                axes[row, 1].text(x1, y1-5, f'{class_name}\\n{score:.2f}', \n",
        "                                 color='red', fontsize=10, weight='bold',\n",
        "                                 bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
        "            \n",
        "            # Store results\n",
        "            result = {\n",
        "                'image_id': img_data['image_id'],\n",
        "                'filename': img_data['filename'],\n",
        "                'description': img_data['description'],\n",
        "                'difficulty': img_data['difficulty'],\n",
        "                'expected_objects': img_data['expected_objects'],\n",
        "                'ground_truth_objects': gt_objects,\n",
        "                'predicted_objects': pred_objects,\n",
        "                'num_predictions': len(pred_objects),\n",
        "                'avg_confidence': float(np.mean(scores)) if len(scores) > 0 else 0.0\n",
        "            }\n",
        "            results.append(result)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save comparison visualization\n",
        "    comparison_viz_path = f\"{CONFIG['output_dir']}/visualizations/comparison_predictions.png\"\n",
        "    plt.savefig(comparison_viz_path, dpi=300, bbox_inches='tight')\n",
        "    logger.info(f\"Comparison visualization saved to {comparison_viz_path}\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Save results\n",
        "    results_path = f\"{CONFIG['output_dir']}/predictions/comparison_results.json\"\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"Comparison results saved to {results_path}\")\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\nüéØ Comparison Images Results Summary:\")\n",
        "    for result in results:\n",
        "        expected = set(result['expected_objects'])\n",
        "        predicted = set(result['predicted_objects'])\n",
        "        overlap = expected.intersection(predicted)\n",
        "        \n",
        "        print(f\"\\nüì∑ {result['filename']} ({result['difficulty']})\")\n",
        "        print(f\"   Expected: {', '.join(expected)}\")\n",
        "        print(f\"   Predicted: {', '.join(predicted)}\")\n",
        "        print(f\"   Overlap: {', '.join(overlap) if overlap else 'None'}\")\n",
        "        print(f\"   Avg Confidence: {result['avg_confidence']:.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Comparison testing functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "## 10. Data Preparation - Convert VOC to COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert VOC dataset to COCO format\n",
        "logger.info(\"Starting data preparation...\")\n",
        "\n",
        "coco_file = f\"{CONFIG['output_dir']}/data/voc2012_coco.json\"\n",
        "\n",
        "print(f\"üîÑ Converting VOC dataset to COCO format...\")\n",
        "print(f\"   VOC root: {CONFIG['voc_root']}\")\n",
        "print(f\"   Output file: {coco_file}\")\n",
        "\n",
        "# Check if VOC dataset exists\n",
        "if not os.path.exists(CONFIG['voc_root']):\n",
        "    logger.error(f\"VOC dataset not found at {CONFIG['voc_root']}\")\n",
        "    print(f\"‚ùå VOC dataset not found at {CONFIG['voc_root']}\")\n",
        "    print(\"Please update the 'voc_root' path in the configuration section.\")\n",
        "else:\n",
        "    # Convert dataset\n",
        "    conversion_stats = convert_voc_to_coco(CONFIG['voc_root'], coco_file, 'trainval')\n",
        "    \n",
        "    if conversion_stats:\n",
        "        print(f\"‚úÖ Conversion completed successfully!\")\n",
        "        print(f\"   Total images: {conversion_stats['total_images']}\")\n",
        "        print(f\"   Total annotations: {conversion_stats['total_annotations']}\")\n",
        "        print(f\"   Skipped images: {conversion_stats['skipped_images']}\")\n",
        "        \n",
        "        CONFIG['coco_file'] = coco_file\n",
        "        CONFIG['image_dir'] = f\"{CONFIG['voc_root']}/JPEGImages\"\n",
        "    else:\n",
        "        print(f\"‚ùå Conversion failed!\")\n",
        "        logger.error(\"VOC to COCO conversion failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "## 11. Dataset and DataLoader Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "logger.info(\"Creating dataset and dataloaders...\")\n",
        "\n",
        "try:\n",
        "    dataset = COCODetectionDataset(\n",
        "        coco_json_path=CONFIG['coco_file'],\n",
        "        image_dir=CONFIG['image_dir'],\n",
        "        transforms=transform\n",
        "    )\n",
        "    \n",
        "    # Split dataset (80% train, 20% val)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=CONFIG['num_workers'],\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if CONFIG['device'] == 'cuda' else False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=CONFIG['num_workers'],\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if CONFIG['device'] == 'cuda' else False\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Dataset created successfully!\")\n",
        "    print(f\"   Total samples: {dataset_size}\")\n",
        "    print(f\"   Training samples: {train_size}\")\n",
        "    print(f\"   Validation samples: {val_size}\")\n",
        "    print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Validation batches: {len(val_loader)}\")\n",
        "    \n",
        "    logger.info(f\"Dataset split - Train: {train_size}, Val: {val_size}\")\n",
        "    logger.info(f\"DataLoaders created - Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create dataset: {e}\")\n",
        "    print(f\"‚ùå Failed to create dataset: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "## 12. Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "logger.info(\"Creating SSD300 model...\")\n",
        "\n",
        "try:\n",
        "    model = create_ssd300_model(num_classes=len(VOC_CLASSES), pretrained=True)\n",
        "    model = model.to(CONFIG['device'])\n",
        "    \n",
        "    # Setup optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    \n",
        "    # Calculate model size\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "    training_metrics['model_size_mb'] = model_size\n",
        "    \n",
        "    print(f\"‚úÖ Model created successfully!\")\n",
        "    print(f\"   Model: SSD300 with VGG16 backbone\")\n",
        "    print(f\"   Device: {CONFIG['device']}\")\n",
        "    print(f\"   Parameters: {training_metrics['total_parameters']:,}\")\n",
        "    print(f\"   Model size: {model_size:.1f} MB\")\n",
        "    print(f\"   Optimizer: Adam (lr={CONFIG['learning_rate']})\")\n",
        "    \n",
        "    logger.info(f\"Model setup completed - Device: {CONFIG['device']}, Size: {model_size:.1f}MB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create model: {e}\")\n",
        "    print(f\"‚ùå Failed to create model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "## 13. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with enhanced monitoring\n",
        "logger.info(\"Starting training...\")\n",
        "training_metrics['start_time'] = datetime.now()\n",
        "\n",
        "print(f\"üöÄ Starting SSD300 training...\")\n",
        "print(f\"   Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"   Device: {CONFIG['device']}\")\n",
        "print(f\"   Estimated time: {CONFIG['num_epochs'] * len(train_loader) * CONFIG['batch_size'] / 100:.1f} - {CONFIG['num_epochs'] * len(train_loader) * CONFIG['batch_size'] / 50:.1f} minutes\")\n",
        "\n",
        "best_loss = float('inf')\n",
        "\n",
        "try:\n",
        "    for epoch in range(CONFIG['num_epochs']):\n",
        "        logger.info(f\"Starting epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "        print(f\"\\nüìä Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "        \n",
        "        # Train for one epoch\n",
        "        epoch_loss = train_one_epoch(model, train_loader, optimizer, CONFIG['device'], epoch)\n",
        "        \n",
        "        # Update learning rate tracking\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        training_metrics['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Save checkpoint\n",
        "        if CONFIG['save_checkpoints'] and (epoch + 1) % CONFIG['checkpoint_every'] == 0:\n",
        "            checkpoint_path = f\"{CONFIG['output_dir']}/models/ssd300_epoch_{epoch + 1}.pth\"\n",
        "            is_best = epoch_loss < best_loss\n",
        "            if is_best:\n",
        "                best_loss = epoch_loss\n",
        "            \n",
        "            save_checkpoint(model, optimizer, epoch + 1, epoch_loss, checkpoint_path, is_best)\n",
        "        \n",
        "        # Print epoch summary\n",
        "        print(f\"   ‚úÖ Epoch {epoch + 1} completed\")\n",
        "        print(f\"   üìâ Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"   ‚è±Ô∏è  Time: {training_metrics['epoch_times'][-1]:.1f}s\")\n",
        "        print(f\"   üìö Learning Rate: {current_lr:.6f}\")\n",
        "        \n",
        "        # Early visualization of training progress\n",
        "        if (epoch + 1) % 2 == 0 or epoch == CONFIG['num_epochs'] - 1:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Training Progress Update - Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "            if len(training_metrics['train_losses']) > 1:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.plot(training_metrics['train_losses'], 'b-', linewidth=2)\n",
        "                plt.title('Training Loss')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.plot(training_metrics['epoch_times'], 'g-', linewidth=2)\n",
        "                plt.title('Epoch Time')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Time (s)')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "    \n",
        "    training_metrics['end_time'] = datetime.now()\n",
        "    total_training_time = training_metrics['end_time'] - training_metrics['start_time']\n",
        "    \n",
        "    print(f\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"   Total time: {total_training_time}\")\n",
        "    print(f\"   Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "    print(f\"   Best loss: {best_loss:.4f}\")\n",
        "    \n",
        "    logger.info(f\"Training completed successfully in {total_training_time}\")\n",
        "    logger.info(f\"Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    training_metrics['end_time'] = datetime.now()\n",
        "    logger.error(f\"Training failed: {e}\")\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-28",
      "metadata": {},
      "source": [
        "## 14. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "logger.info(\"Starting model evaluation...\")\n",
        "\n",
        "print(f\"üîç Evaluating trained model...\")\n",
        "\n",
        "try:\n",
        "    # Load ground truth COCO for evaluation\n",
        "    coco_gt = COCO(CONFIG['coco_file'])\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    eval_metrics = evaluate_model(model, val_loader, CONFIG['device'], coco_gt)\n",
        "    \n",
        "    if eval_metrics:\n",
        "        training_metrics['final_metrics'] = eval_metrics\n",
        "        training_metrics['best_map'] = eval_metrics.get('mAP_0.5', 0.0)\n",
        "        \n",
        "        print(f\"\\nüìä Evaluation Results:\")\n",
        "        print(f\"   mAP@0.5-0.95: {eval_metrics.get('mAP_0.5_0.95', 0):.4f}\")\n",
        "        print(f\"   mAP@0.5: {eval_metrics.get('mAP_0.5', 0):.4f}\")\n",
        "        print(f\"   mAP@0.75: {eval_metrics.get('mAP_0.75', 0):.4f}\")\n",
        "        print(f\"   AR@100: {eval_metrics.get('AR_100', 0):.4f}\")\n",
        "        \n",
        "        logger.info(f\"Evaluation completed - mAP@0.5: {eval_metrics.get('mAP_0.5', 0):.4f}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Evaluation completed but no metrics generated\")\n",
        "        logger.warning(\"Evaluation completed but no metrics generated\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logger.error(f\"Evaluation failed: {e}\")\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-30",
      "metadata": {},
      "source": [
        "## 15. Training Progress Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training visualization\n",
        "if CONFIG['save_visualizations']:\n",
        "    logger.info(\"Creating training progress visualization...\")\n",
        "    \n",
        "    viz_path = f\"{CONFIG['output_dir']}/visualizations/training_progress.png\"\n",
        "    visualize_training_progress(training_metrics, viz_path)\n",
        "    \n",
        "    print(f\"‚úÖ Training visualization saved to {viz_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-32",
      "metadata": {},
      "source": [
        "## 16. Sample Predictions Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample predictions\n",
        "if CONFIG['save_visualizations']:\n",
        "    logger.info(\"Creating sample predictions visualization...\")\n",
        "    \n",
        "    pred_viz_path = f\"{CONFIG['output_dir']}/visualizations/sample_predictions.png\"\n",
        "    visualize_predictions(model, dataset, CONFIG['device'], num_images=4, save_path=pred_viz_path)\n",
        "    \n",
        "    print(f\"‚úÖ Sample predictions visualization saved to {pred_viz_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-34",
      "metadata": {},
      "source": [
        "## 17. Comparison Images Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on comparison images\n",
        "if CONFIG['run_comparison_images']:\n",
        "    logger.info(\"Testing on comparison images...\")\n",
        "    \n",
        "    print(f\"üñºÔ∏è  Testing on comparison images...\")\n",
        "    \n",
        "    # Try to find comparison_images.json in common locations\n",
        "    comparison_file_paths = [\n",
        "        '/kaggle/input/comparison-images/comparison_images.json',\n",
        "        '/kaggle/working/comparison_images.json',\n",
        "        './comparison_images.json',\n",
        "        '../comparison_images.json'\n",
        "    ]\n",
        "    \n",
        "    comparison_file = None\n",
        "    for path in comparison_file_paths:\n",
        "        if os.path.exists(path):\n",
        "            comparison_file = path\n",
        "            break\n",
        "    \n",
        "    if comparison_file:\n",
        "        print(f\"üìÅ Found comparison file: {comparison_file}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No comparison_images.json found, using default images\")\n",
        "    \n",
        "    try:\n",
        "        comparison_results = test_comparison_images(model, dataset, CONFIG['device'], comparison_file)\n",
        "        \n",
        "        print(f\"\\n‚úÖ Comparison testing completed!\")\n",
        "        print(f\"   Tested images: {len(comparison_results)}\")\n",
        "        \n",
        "        logger.info(f\"Comparison testing completed on {len(comparison_results)} images\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Comparison testing failed: {e}\")\n",
        "        print(f\"‚ùå Comparison testing failed: {e}\")\n",
        "else:\n",
        "    print(f\"‚è≠Ô∏è  Skipping comparison images testing (disabled in config)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-36",
      "metadata": {},
      "source": [
        "## 18. Final Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive final report\n",
        "logger.info(\"Generating final report...\")\n",
        "\n",
        "final_report = {\n",
        "    'experiment_info': {\n",
        "        'name': CONFIG['experiment_name'],\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model': 'SSD300',\n",
        "        'backbone': 'VGG16',\n",
        "        'dataset': 'Pascal VOC 2012',\n",
        "        'device': CONFIG['device']\n",
        "    },\n",
        "    'configuration': CONFIG,\n",
        "    'training_metrics': training_metrics,\n",
        "    'dataset_info': {\n",
        "        'total_samples': len(dataset) if 'dataset' in locals() else 0,\n",
        "        'train_samples': len(train_dataset) if 'train_dataset' in locals() else 0,\n",
        "        'val_samples': len(val_dataset) if 'val_dataset' in locals() else 0,\n",
        "        'num_classes': len(VOC_CLASSES),\n",
        "        'classes': VOC_CLASSES\n",
        "    },\n",
        "    'model_info': {\n",
        "        'total_parameters': training_metrics['total_parameters'],\n",
        "        'model_size_mb': training_metrics['model_size_mb'],\n",
        "        'optimizer': 'Adam',\n",
        "        'learning_rate': CONFIG['learning_rate']\n",
        "    },\n",
        "    'output_files': {\n",
        "        'log_file': f\"{CONFIG['output_dir']}/logs/{CONFIG['experiment_name']}.log\",\n",
        "        'final_model': f\"{CONFIG['output_dir']}/models/ssd300_epoch_{CONFIG['num_epochs']}.pth\",\n",
        "        'best_model': f\"{CONFIG['output_dir']}/models/ssd300_epoch_{CONFIG['num_epochs']}_best.pth\",\n",
        "        'predictions': f\"{CONFIG['output_dir']}/predictions/ssd300_predictions.json\",\n",
        "        'comparison_results': f\"{CONFIG['output_dir']}/predictions/comparison_results.json\",\n",
        "        'training_viz': f\"{CONFIG['output_dir']}/visualizations/training_progress.png\",\n",
        "        'predictions_viz': f\"{CONFIG['output_dir']}/visualizations/sample_predictions.png\",\n",
        "        'comparison_viz': f\"{CONFIG['output_dir']}/visualizations/comparison_predictions.png\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save final report\n",
        "report_file = f\"{CONFIG['output_dir']}/{CONFIG['experiment_name']}_final_report.json\"\n",
        "with open(report_file, 'w') as f:\n",
        "    json.dump(final_report, f, indent=2, default=str)\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{CONFIG['output_dir']}/models/ssd300_final.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'training_metrics': training_metrics,\n",
        "    'final_report': final_report\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"\\nüéâ SSD300 Pipeline Completed Successfully!\")\n",
        "print(f\"\\nüìã Final Summary:\")\n",
        "print(f\"   Experiment: {CONFIG['experiment_name']}\")\n",
        "print(f\"   Device: {CONFIG['device']}\")\n",
        "print(f\"   Training time: {training_metrics['end_time'] - training_metrics['start_time']}\")\n",
        "print(f\"   Epochs completed: {len(training_metrics['train_losses'])}/{CONFIG['num_epochs']}\")\n",
        "print(f\"   Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "if training_metrics['best_map'] > 0:\n",
        "    print(f\"   Best mAP@0.5: {training_metrics['best_map']:.4f}\")\n",
        "print(f\"   Model size: {training_metrics['model_size_mb']:.1f} MB\")\n",
        "print(f\"   Parameters: {training_metrics['total_parameters']:,}\")\n",
        "\n",
        "print(f\"\\nüìÅ Output Files:\")\n",
        "print(f\"   üìä Final report: {report_file}\")\n",
        "print(f\"   ü§ñ Final model: {final_model_path}\")\n",
        "print(f\"   üìù Training log: {log_file}\")\n",
        "print(f\"   üìà Visualizations: {CONFIG['output_dir']}/visualizations/\")\n",
        "print(f\"   üéØ Predictions: {CONFIG['output_dir']}/predictions/\")\n",
        "\n",
        "logger.info(f\"SSD300 pipeline completed successfully!\")\n",
        "logger.info(f\"Final report saved to {report_file}\")\n",
        "logger.info(f\"Final model saved to {final_model_path}\")\n",
        "logger.info(\"=== EXPERIMENT COMPLETED ===\")\n",
        "\n",
        "print(f\"\\n‚ú® All done! Check the output directory for all generated files.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}