{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 Enhanced Training and Evaluation Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for training and evaluating YOLOv5 on Pascal VOC 2012 dataset with enhanced logging, visualization, and comparison capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import subprocess\n",
    "import yaml\n",
    "from IPython.display import Image, display, clear_output\n",
    "import time\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {Path().resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and model configuration\n",
    "CONFIG = {\n",
    "    'voc_dataset_path': project_root / 'data' / 'VOCdevkit' / 'VOC2012',\n",
    "    'yolo_dataset_path': project_root / 'YOLOv5_custom' / 'data' / 'VOC_YOLO',\n",
    "    'yolov5_repo_path': project_root / 'yolov5',\n",
    "    'output_path': project_root / 'YOLOv5_custom' / 'outputs',\n",
    "    'comparison_file': project_root / 'comparison_images.json',\n",
    "    \n",
    "    # Training parameters\n",
    "    'img_size': 640,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'weights': 'yolov5s.pt',\n",
    "    'conf_threshold': 0.25,\n",
    "    'iou_threshold': 0.45,\n",
    "    'device': '',  # '' for auto-detect, '0' for GPU 0, 'cpu' for CPU\n",
    "    \n",
    "    # Experiment settings\n",
    "    'experiment_name': f'yolov5_voc_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'save_visualizations': True,\n",
    "    'run_comparison': True\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['output_path'].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG['output_path'] / 'logs').mkdir(exist_ok=True)\n",
    "(CONFIG['output_path'] / 'models').mkdir(exist_ok=True)\n",
    "(CONFIG['output_path'] / 'predictions').mkdir(exist_ok=True)\n",
    "(CONFIG['output_path'] / 'visualizations').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Enhanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup enhanced logging\n",
    "log_file = CONFIG['output_path'] / 'logs' / f'{CONFIG[\"experiment_name\"]}.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Starting YOLOv5 experiment: {CONFIG['experiment_name']}\")\n",
    "logger.info(f\"Log file: {log_file}\")\n",
    "\n",
    "# Training metrics tracking\n",
    "training_metrics = {\n",
    "    'start_time': None,\n",
    "    'end_time': None,\n",
    "    'total_time': None,\n",
    "    'epochs_completed': 0,\n",
    "    'best_map': 0.0,\n",
    "    'final_metrics': {}\n",
    "}\n",
    "\n",
    "print(f\"Enhanced logging setup complete. Log file: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation - Convert VOC to YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert VOC dataset to YOLO format\n",
    "logger.info(\"Starting VOC to YOLO conversion...\")\n",
    "\n",
    "# Import conversion script\n",
    "sys.path.append(str(project_root / 'YOLOv5_custom' / 'src'))\n",
    "\n",
    "# Run conversion using subprocess for better control\n",
    "conversion_cmd = [\n",
    "    sys.executable,\n",
    "    str(project_root / 'YOLOv5_custom' / 'src' / 'voc_to_yolo.py'),\n",
    "    '--voc-root', str(CONFIG['voc_dataset_path']),\n",
    "    '--output-root', str(CONFIG['yolo_dataset_path']),\n",
    "    '--splits', 'train', 'val',\n",
    "    '--create-config'\n",
    "]\n",
    "\n",
    "logger.info(f\"Running conversion command: {' '.join(conversion_cmd)}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(conversion_cmd, capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        logger.info(\"VOC to YOLO conversion completed successfully!\")\n",
    "        print(\"‚úÖ Dataset conversion successful!\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        logger.error(f\"Conversion failed with return code {result.returncode}\")\n",
    "        logger.error(f\"Error output: {result.stderr}\")\n",
    "        print(\"‚ùå Dataset conversion failed!\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    logger.error(\"Conversion timed out\")\n",
    "    print(\"‚ùå Conversion timed out!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Conversion failed with exception: {e}\")\n",
    "    print(f\"‚ùå Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Dataset Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and statistics\n",
    "yolo_data_path = CONFIG['yolo_dataset_path']\n",
    "\n",
    "if yolo_data_path.exists():\n",
    "    print(\"üìä Dataset Structure:\")\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        images_dir = yolo_data_path / 'images' / split\n",
    "        labels_dir = yolo_data_path / 'labels' / split\n",
    "        \n",
    "        if images_dir.exists() and labels_dir.exists():\n",
    "            num_images = len(list(images_dir.glob('*.jpg')))\n",
    "            num_labels = len(list(labels_dir.glob('*.txt')))\n",
    "            print(f\"  {split}: {num_images} images, {num_labels} label files\")\n",
    "        else:\n",
    "            print(f\"  {split}: Missing directories\")\n",
    "    \n",
    "    # Check for config file\n",
    "    config_file = yolo_data_path / 'yolov5_voc_config.yaml'\n",
    "    if config_file.exists():\n",
    "        print(f\"  ‚úÖ YOLOv5 config file: {config_file}\")\n",
    "        CONFIG['data_config'] = str(config_file)\n",
    "    else:\n",
    "        print(\"  ‚ùå YOLOv5 config file not found\")\n",
    "    \n",
    "    # Load and display conversion report if available\n",
    "    report_file = yolo_data_path / 'conversion_report.json'\n",
    "    if report_file.exists():\n",
    "        with open(report_file, 'r') as f:\n",
    "            report = json.load(f)\n",
    "        \n",
    "        print(\"\\nüìà Conversion Statistics:\")\n",
    "        stats = report['statistics']\n",
    "        print(f\"  Total images: {stats['total_images']}\")\n",
    "        print(f\"  Converted: {stats['converted_images']}\")\n",
    "        print(f\"  Failed: {stats['failed_conversions']}\")\n",
    "        \n",
    "        for split, split_stats in stats['splits'].items():\n",
    "            print(f\"  {split}: {split_stats['converted']}/{split_stats['total']}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå YOLO dataset directory not found!\")\n",
    "    logger.error(f\"YOLO dataset directory not found: {yolo_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. YOLOv5 Training with Enhanced Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start YOLOv5 training with enhanced monitoring\n",
    "logger.info(\"Starting YOLOv5 training...\")\n",
    "training_metrics['start_time'] = datetime.now()\n",
    "\n",
    "# Prepare training command\n",
    "train_cmd = [\n",
    "    sys.executable,\n",
    "    str(CONFIG['yolov5_repo_path'] / 'train.py'),\n",
    "    '--img', str(CONFIG['img_size']),\n",
    "    '--batch', str(CONFIG['batch_size']),\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--data', CONFIG['data_config'],\n",
    "    '--weights', CONFIG['weights'],\n",
    "    '--name', CONFIG['experiment_name'],\n",
    "    '--project', str(CONFIG['output_path'] / 'runs'),\n",
    "    '--exist-ok'\n",
    "]\n",
    "\n",
    "if CONFIG['device']:\n",
    "    train_cmd.extend(['--device', CONFIG['device']])\n",
    "\n",
    "logger.info(f\"Training command: {' '.join(train_cmd)}\")\n",
    "print(f\"üöÄ Starting training with command:\")\n",
    "print(f\"   {' '.join(train_cmd)}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {CONFIG['epochs'] * 2} - {CONFIG['epochs'] * 5} minutes\")\n",
    "\n",
    "try:\n",
    "    # Run training\n",
    "    process = subprocess.Popen(\n",
    "        train_cmd,\n",
    "        cwd=str(CONFIG['yolov5_repo_path']),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "    \n",
    "    # Monitor training progress\n",
    "    epoch_times = []\n",
    "    current_epoch = 0\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # Log all output\n",
    "            logger.info(f\"TRAINING: {line}\")\n",
    "            \n",
    "            # Parse epoch information\n",
    "            if 'Epoch' in line and '/' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if part == 'Epoch' and i + 1 < len(parts):\n",
    "                            epoch_info = parts[i + 1]\n",
    "                            if '/' in epoch_info:\n",
    "                                current_epoch = int(epoch_info.split('/')[0])\n",
    "                                training_metrics['epochs_completed'] = current_epoch\n",
    "                                print(f\"üìä Epoch {current_epoch}/{CONFIG['epochs']} in progress...\")\n",
    "                                break\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Parse metrics\n",
    "            if 'mAP@0.5' in line:\n",
    "                try:\n",
    "                    # Extract mAP value\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if 'mAP@0.5' in part and i + 1 < len(parts):\n",
    "                            map_value = float(parts[i + 1])\n",
    "                            if map_value > training_metrics['best_map']:\n",
    "                                training_metrics['best_map'] = map_value\n",
    "                                print(f\"üéØ New best mAP@0.5: {map_value:.4f}\")\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Wait for process to complete\n",
    "    return_code = process.wait()\n",
    "    \n",
    "    training_metrics['end_time'] = datetime.now()\n",
    "    training_metrics['total_time'] = training_metrics['end_time'] - training_metrics['start_time']\n",
    "    \n",
    "    if return_code == 0:\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Total training time: {training_metrics['total_time']}\")\n",
    "        print(f\"üéØ Best mAP@0.5: {training_metrics['best_map']:.4f}\")\n",
    "    else:\n",
    "        logger.error(f\"Training failed with return code {return_code}\")\n",
    "        print(\"‚ùå Training failed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed with exception: {e}\")\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    training_metrics['end_time'] = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "results_dir = CONFIG['output_path'] / 'runs' / 'train' / CONFIG['experiment_name']\n",
    "\n",
    "if results_dir.exists():\n",
    "    print(f\"üìä Training Results Analysis\")\n",
    "    print(f\"Results directory: {results_dir}\")\n",
    "    \n",
    "    # Load training results\n",
    "    results_file = results_dir / 'results.csv'\n",
    "    if results_file.exists():\n",
    "        df = pd.read_csv(results_file)\n",
    "        df.columns = df.columns.str.strip()  # Clean column names\n",
    "        \n",
    "        print(f\"\\nüìà Final Training Metrics (Epoch {len(df)}):\") \n",
    "        if len(df) > 0:\n",
    "            final_row = df.iloc[-1]\n",
    "            metrics_to_show = {\n",
    "                'Precision': 'metrics/precision',\n",
    "                'Recall': 'metrics/recall', \n",
    "                'mAP@0.5': 'metrics/mAP_0.5',\n",
    "                'mAP@0.5:0.95': 'metrics/mAP_0.5:0.95'\n",
    "            }\n",
    "            \n",
    "            for display_name, col_name in metrics_to_show.items():\n",
    "                if col_name in df.columns:\n",
    "                    value = final_row[col_name]\n",
    "                    print(f\"  {display_name}: {value:.4f}\")\n",
    "                    training_metrics['final_metrics'][display_name] = value\n",
    "        \n",
    "        # Create training progress visualization\n",
    "        if CONFIG['save_visualizations']:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            fig.suptitle(f'YOLOv5 Training Progress - {CONFIG[\"experiment_name\"]}', fontsize=16)\n",
    "            \n",
    "            # Plot 1: Loss curves\n",
    "            ax1 = axes[0, 0]\n",
    "            loss_cols = [col for col in df.columns if 'loss' in col.lower()]\n",
    "            for col in loss_cols[:3]:  # Show first 3 loss columns\n",
    "                if col in df.columns:\n",
    "                    ax1.plot(df.index, df[col], label=col.replace('train/', '').replace('val/', ''))\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot 2: mAP metrics\n",
    "            ax2 = axes[0, 1]\n",
    "            map_cols = [col for col in df.columns if 'mAP' in col]\n",
    "            for col in map_cols:\n",
    "                ax2.plot(df.index, df[col], label=col.replace('metrics/', ''))\n",
    "            ax2.set_title('mAP Metrics')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('mAP')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            # Plot 3: Precision and Recall\n",
    "            ax3 = axes[1, 0]\n",
    "            if 'metrics/precision' in df.columns:\n",
    "                ax3.plot(df.index, df['metrics/precision'], label='Precision', color='blue')\n",
    "            if 'metrics/recall' in df.columns:\n",
    "                ax3.plot(df.index, df['metrics/recall'], label='Recall', color='red')\n",
    "            ax3.set_title('Precision and Recall')\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Score')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            # Plot 4: Learning Rate\n",
    "            ax4 = axes[1, 1]\n",
    "            lr_cols = [col for col in df.columns if 'lr' in col.lower()]\n",
    "            for col in lr_cols:\n",
    "                ax4.plot(df.index, df[col], label=col)\n",
    "            ax4.set_title('Learning Rate Schedule')\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('Learning Rate')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save visualization\n",
    "            viz_path = CONFIG['output_path'] / 'visualizations' / 'training_analysis.png'\n",
    "            plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Training visualization saved to {viz_path}\")\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Training results file not found\")\n",
    "        logger.warning(f\"Results file not found: {results_file}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Training results directory not found\")\n",
    "    logger.warning(f\"Results directory not found: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model\n",
    "logger.info(\"Starting model evaluation...\")\n",
    "\n",
    "# Find best weights\n",
    "weights_dir = results_dir / 'weights'\n",
    "best_weights = weights_dir / 'best.pt'\n",
    "last_weights = weights_dir / 'last.pt'\n",
    "\n",
    "if best_weights.exists():\n",
    "    model_weights = str(best_weights)\n",
    "    print(f\"üéØ Using best weights: {best_weights}\")\n",
    "elif last_weights.exists():\n",
    "    model_weights = str(last_weights)\n",
    "    print(f\"üì¶ Using last weights: {last_weights}\")\n",
    "else:\n",
    "    print(\"‚ùå No trained weights found!\")\n",
    "    model_weights = None\n",
    "\n",
    "if model_weights:\n",
    "    # Prepare evaluation command\n",
    "    eval_cmd = [\n",
    "        sys.executable,\n",
    "        str(CONFIG['yolov5_repo_path'] / 'val.py'),\n",
    "        '--data', CONFIG['data_config'],\n",
    "        '--weights', model_weights,\n",
    "        '--img', str(CONFIG['img_size']),\n",
    "        '--batch', str(CONFIG['batch_size']),\n",
    "        '--conf', str(CONFIG['conf_threshold']),\n",
    "        '--iou', str(CONFIG['iou_threshold']),\n",
    "        '--project', str(CONFIG['output_path'] / 'validation'),\n",
    "        '--name', CONFIG['experiment_name'],\n",
    "        '--save-txt', '--save-conf', '--save-json',\n",
    "        '--exist-ok'\n",
    "    ]\n",
    "    \n",
    "    if CONFIG['device']:\n",
    "        eval_cmd.extend(['--device', CONFIG['device']])\n",
    "    \n",
    "    logger.info(f\"Evaluation command: {' '.join(eval_cmd)}\")\n",
    "    print(f\"üîç Running evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            eval_cmd,\n",
    "            cwd=str(CONFIG['yolov5_repo_path']),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=1800  # 30 minutes timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            logger.info(\"Evaluation completed successfully!\")\n",
    "            print(\"‚úÖ Evaluation completed successfully!\")\n",
    "            print(\"\\nüìä Evaluation Output:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Parse evaluation metrics\n",
    "            eval_metrics = {}\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'mAP@.5' in line and 'all' in line:\n",
    "                    # Parse mAP values from output\n",
    "                    parts = line.split()\n",
    "                    try:\n",
    "                        for i, part in enumerate(parts):\n",
    "                            if part == 'all' and i + 4 < len(parts):\n",
    "                                eval_metrics['mAP_0.5'] = float(parts[i + 3])\n",
    "                                eval_metrics['mAP_0.5_0.95'] = float(parts[i + 4])\n",
    "                                break\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "            \n",
    "            if eval_metrics:\n",
    "                print(f\"\\nüéØ Final Evaluation Metrics:\")\n",
    "                for metric, value in eval_metrics.items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "                    training_metrics['final_metrics'][f'eval_{metric}'] = value\n",
    "            \n",
    "        else:\n",
    "            logger.error(f\"Evaluation failed with return code {result.returncode}\")\n",
    "            logger.error(f\"Error output: {result.stderr}\")\n",
    "            print(\"‚ùå Evaluation failed!\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(\"Evaluation timed out\")\n",
    "        print(\"‚ùå Evaluation timed out!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed with exception: {e}\")\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison Image Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on comparison images\n",
    "if CONFIG['run_comparison'] and CONFIG['comparison_file'].exists() and model_weights:\n",
    "    logger.info(\"Running inference on comparison images...\")\n",
    "    \n",
    "    # Load comparison images\n",
    "    with open(CONFIG['comparison_file'], 'r') as f:\n",
    "        comparison_data = json.load(f)\n",
    "    \n",
    "    print(f\"üñºÔ∏è  Testing on {len(comparison_data.get('images', []))} comparison images...\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for i, img_data in enumerate(comparison_data.get('images', [])):\n",
    "        # Find image in dataset\n",
    "        img_path = None\n",
    "        for split in ['train', 'val']:\n",
    "            potential_path = CONFIG['yolo_dataset_path'] / 'images' / split / img_data['filename']\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if not img_path:\n",
    "            logger.warning(f\"Comparison image not found: {img_data['filename']}\")\n",
    "            continue\n",
    "        \n",
    "        # Run detection\n",
    "        detect_cmd = [\n",
    "            sys.executable,\n",
    "            str(CONFIG['yolov5_repo_path'] / 'detect.py'),\n",
    "            '--weights', model_weights,\n",
    "            '--source', str(img_path),\n",
    "            '--img', str(CONFIG['img_size']),\n",
    "            '--conf', str(CONFIG['conf_threshold']),\n",
    "            '--iou', str(CONFIG['iou_threshold']),\n",
    "            '--project', str(CONFIG['output_path'] / 'comparison_inference'),\n",
    "            '--name', f\"{CONFIG['experiment_name']}_{img_data['image_id']}\",\n",
    "            '--save-txt', '--save-conf',\n",
    "            '--exist-ok'\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                detect_cmd,\n",
    "                cwd=str(CONFIG['yolov5_repo_path']),\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                comparison_result = {\n",
    "                    'image_id': img_data['image_id'],\n",
    "                    'filename': img_data['filename'], \n",
    "                    'expected_objects': img_data.get('expected_objects', []),\n",
    "                    'difficulty': img_data.get('difficulty', 'unknown'),\n",
    "                    'inference_success': True\n",
    "                }\n",
    "                comparison_results.append(comparison_result)\n",
    "                print(f\"  ‚úÖ {img_data['filename']} - {img_data.get('difficulty', 'unknown')}\")\n",
    "            else:\n",
    "                logger.warning(f\"Detection failed for {img_data['filename']}\")\n",
    "                print(f\"  ‚ùå {img_data['filename']} - Detection failed\")\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            logger.warning(f\"Detection timeout for {img_data['filename']}\")\n",
    "            print(f\"  ‚è±Ô∏è  {img_data['filename']} - Timeout\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Detection error for {img_data['filename']}: {e}\")\n",
    "            print(f\"  ‚ùå {img_data['filename']} - Error: {e}\")\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_results_file = CONFIG['output_path'] / 'predictions' / f'{CONFIG[\"experiment_name\"]}_comparison_results.json'\n",
    "    with open(comparison_results_file, 'w') as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìä Comparison Results Summary:\")\n",
    "    print(f\"  Total images tested: {len(comparison_results)}\")\n",
    "    print(f\"  Successful inferences: {sum(1 for r in comparison_results if r['inference_success'])}\")\n",
    "    print(f\"  Results saved to: {comparison_results_file}\")\n",
    "    \n",
    "    logger.info(f\"Comparison testing completed. Results saved to {comparison_results_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping comparison image testing (disabled or no weights available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive final report\n",
    "logger.info(\"Generating final report...\")\n",
    "\n",
    "final_report = {\n",
    "    'experiment_info': {\n",
    "        'name': CONFIG['experiment_name'],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': 'YOLOv5',\n",
    "        'dataset': 'Pascal VOC 2012'\n",
    "    },\n",
    "    'configuration': {\n",
    "        'img_size': CONFIG['img_size'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'weights': CONFIG['weights'],\n",
    "        'conf_threshold': CONFIG['conf_threshold'],\n",
    "        'iou_threshold': CONFIG['iou_threshold']\n",
    "    },\n",
    "    'training_metrics': training_metrics,\n",
    "    'model_files': {\n",
    "        'best_weights': str(best_weights) if best_weights.exists() else None,\n",
    "        'last_weights': str(last_weights) if last_weights.exists() else None,\n",
    "        'results_csv': str(results_file) if results_file.exists() else None\n",
    "    },\n",
    "    'output_files': {\n",
    "        'log_file': str(log_file),\n",
    "        'training_visualization': str(CONFIG['output_path'] / 'visualizations' / 'training_analysis.png'),\n",
    "        'comparison_results': str(CONFIG['output_path'] / 'predictions' / f'{CONFIG[\"experiment_name\"]}_comparison_results.json')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save final report\n",
    "report_file = CONFIG['output_path'] / f'{CONFIG[\"experiment_name\"]}_final_report.json'\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(final_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nüéâ YOLOv5 Pipeline Complete!\")\n",
    "print(\"\\nüìã Final Summary:\")\n",
    "print(f\"  Experiment: {CONFIG['experiment_name']}\")\n",
    "if training_metrics['total_time']:\n",
    "    print(f\"  Training time: {training_metrics['total_time']}\")\n",
    "print(f\"  Epochs completed: {training_metrics['epochs_completed']}/{CONFIG['epochs']}\")\n",
    "if training_metrics['best_map']:\n",
    "    print(f\"  Best mAP@0.5: {training_metrics['best_map']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"  Final report: {report_file}\")\n",
    "print(f\"  Training log: {log_file}\")\n",
    "if best_weights.exists():\n",
    "    print(f\"  Best model: {best_weights}\")\n",
    "if (CONFIG['output_path'] / 'visualizations' / 'training_analysis.png').exists():\n",
    "    print(f\"  Training visualization: {CONFIG['output_path'] / 'visualizations' / 'training_analysis.png'}\")\n",
    "\n",
    "logger.info(f\"YOLOv5 pipeline completed! Final report saved to {report_file}\")\n",
    "logger.info(\"=== EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Training Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key training visualizations from YOLOv5\n",
    "viz_files = {\n",
    "    'Confusion Matrix': results_dir / 'confusion_matrix.png',\n",
    "    'P-R Curve': results_dir / 'PR_curve.png', \n",
    "    'F1 Curve': results_dir / 'F1_curve.png',\n",
    "    'Results Plot': results_dir / 'results.png',\n",
    "    'Labels Distribution': results_dir / 'labels.jpg'\n",
    "}\n",
    "\n",
    "print(\"üñºÔ∏è  Training Visualizations:\")\n",
    "for title, viz_file in viz_files.items():\n",
    "    if viz_file.exists():\n",
    "        print(f\"\\nüìä {title}:\")\n",
    "        display(Image(str(viz_file)))\n",
    "    else:\n",
    "        print(f\"‚ùå {title}: File not found - {viz_file}\")\n",
    "\n",
    "# Display sample training batches\n",
    "print(f\"\\nüéØ Sample Training Batches:\")\n",
    "for i in range(3):\n",
    "    batch_file = results_dir / f'train_batch{i}.jpg'\n",
    "    if batch_file.exists():\n",
    "        print(f\"\\nüì∑ Training Batch {i}:\")\n",
    "        display(Image(str(batch_file)))\n",
    "\n",
    "# Display sample validation predictions  \n",
    "print(f\"\\nüîç Sample Validation Predictions:\")\n",
    "for i in range(3):\n",
    "    pred_file = results_dir / f'val_batch{i}_pred.jpg'\n",
    "    if pred_file.exists():\n",
    "        print(f\"\\nüéØ Validation Batch {i} Predictions:\")\n",
    "        display(Image(str(pred_file)))"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}