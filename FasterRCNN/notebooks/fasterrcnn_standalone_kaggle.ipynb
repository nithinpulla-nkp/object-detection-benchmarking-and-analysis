{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Faster R-CNN Complete Standalone Pipeline for Kaggle\n",
        "\n",
        "This notebook contains everything needed to train and evaluate Faster R-CNN on Pascal VOC 2012 dataset.\n",
        "It's designed to run standalone in Kaggle without external dependencies.\n",
        "\n",
        "## Features:\n",
        "- Complete VOC to COCO conversion\n",
        "- Faster R-CNN training with enhanced logging\n",
        "- Comprehensive evaluation with COCO metrics\n",
        "- Visual comparisons of ground truth vs predictions\n",
        "- Model checkpointing and visualization\n",
        "- Comparison images testing from comparison_images.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ML/DL libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# COCO evaluation\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# Configure matplotlib and seaborn\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION - MODIFY THESE PATHS FOR YOUR SETUP =====\n",
        "\n",
        "# For Kaggle, update these paths to match your Kaggle setup\n",
        "CONFIG = {\n",
        "    # Dataset paths (modify for Kaggle)\n",
        "    'voc_root': '/kaggle/input/voc2012/VOCdevkit/VOC2012',  # Kaggle input path\n",
        "    'output_dir': '/kaggle/working/fasterrcnn_outputs',  # Kaggle working directory\n",
        "    \n",
        "    # Training parameters\n",
        "    'batch_size': 2,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.005,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 0.0005,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_workers': 4,\n",
        "    \n",
        "    # Evaluation parameters\n",
        "    'conf_threshold': 0.5,\n",
        "    'nms_threshold': 0.5,\n",
        "    \n",
        "    # Experiment settings\n",
        "    'experiment_name': f'fasterrcnn_voc_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
        "    'save_checkpoints': True,\n",
        "    'checkpoint_every': 1,  # Save every N epochs\n",
        "    'save_visualizations': True,\n",
        "    'run_comparison_images': True\n",
        "}\n",
        "\n",
        "# VOC Classes\n",
        "VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "\n",
        "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(VOC_CLASSES)}\n",
        "IDX_TO_CLASS = {idx: cls for idx, cls in enumerate(VOC_CLASSES)}\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/logs\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/predictions\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/visualizations\", exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/data\", exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüìÅ Output directory created: {CONFIG['output_dir']}\")\n",
        "print(f\"üéØ Experiment name: {CONFIG['experiment_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Enhanced Logging Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup comprehensive logging\n",
        "log_file = f\"{CONFIG['output_dir']}/logs/{CONFIG['experiment_name']}.log\"\n",
        "\n",
        "# Create custom logger\n",
        "logger = logging.getLogger('FasterRCNN_Pipeline')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Clear existing handlers\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "# File handler\n",
        "file_handler = logging.FileHandler(log_file)\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Console handler\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Training metrics tracking\n",
        "training_metrics = {\n",
        "    'start_time': None,\n",
        "    'end_time': None,\n",
        "    'epoch_times': [],\n",
        "    'train_losses': [],\n",
        "    'learning_rates': [],\n",
        "    'best_map': 0.0,\n",
        "    'final_metrics': {},\n",
        "    'total_parameters': 0,\n",
        "    'model_size_mb': 0\n",
        "}\n",
        "\n",
        "logger.info(f\"Starting Faster R-CNN experiment: {CONFIG['experiment_name']}\")\n",
        "logger.info(f\"Configuration: {CONFIG}\")\n",
        "logger.info(f\"Log file: {log_file}\")\n",
        "\n",
        "print(f\"‚úÖ Enhanced logging setup complete\")\n",
        "print(f\"üìù Log file: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. VOC to COCO Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_voc_to_coco(voc_root, output_file, image_set='trainval'):\n",
        "    \"\"\"\n",
        "    Convert Pascal VOC dataset to COCO format\n",
        "    \"\"\"\n",
        "    logger.info(f\"Converting VOC to COCO format for {image_set} set...\")\n",
        "    \n",
        "    voc_path = Path(voc_root)\n",
        "    \n",
        "    # Initialize COCO format structure\n",
        "    coco_format = {\n",
        "        \"info\": {\n",
        "            \"description\": \"Pascal VOC 2012 in COCO format\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2012,\n",
        "            \"contributor\": \"Faster R-CNN Pipeline\",\n",
        "            \"date_created\": datetime.now().isoformat()\n",
        "        },\n",
        "        \"licenses\": [{\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Pascal VOC License\",\n",
        "            \"url\": \"http://host.robots.ox.ac.uk/pascal/VOC/\"\n",
        "        }],\n",
        "        \"categories\": [],\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "    \n",
        "    # Add categories\n",
        "    for idx, class_name in enumerate(VOC_CLASSES):\n",
        "        coco_format[\"categories\"].append({\n",
        "            \"id\": idx + 1,  # COCO categories start from 1\n",
        "            \"name\": class_name,\n",
        "            \"supercategory\": \"object\"\n",
        "        })\n",
        "    \n",
        "    # Read image IDs\n",
        "    image_set_file = voc_path / 'ImageSets' / 'Main' / f'{image_set}.txt'\n",
        "    if not image_set_file.exists():\n",
        "        logger.error(f\"Image set file not found: {image_set_file}\")\n",
        "        return False\n",
        "    \n",
        "    with open(image_set_file, 'r') as f:\n",
        "        image_ids = [line.strip() for line in f.readlines()]\n",
        "    \n",
        "    annotation_id = 1\n",
        "    conversion_stats = {'total_images': 0, 'total_annotations': 0, 'skipped_images': 0}\n",
        "    \n",
        "    logger.info(f\"Processing {len(image_ids)} images...\")\n",
        "    \n",
        "    for idx, image_id in enumerate(image_ids):\n",
        "        if idx % 500 == 0:\n",
        "            logger.info(f\"Processed {idx}/{len(image_ids)} images\")\n",
        "        \n",
        "        # Image file\n",
        "        img_file = voc_path / 'JPEGImages' / f'{image_id}.jpg'\n",
        "        if not img_file.exists():\n",
        "            logger.warning(f\"Image file not found: {img_file}\")\n",
        "            conversion_stats['skipped_images'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Get image dimensions\n",
        "        try:\n",
        "            with Image.open(img_file) as img:\n",
        "                width, height = img.size\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Cannot read image {img_file}: {e}\")\n",
        "            conversion_stats['skipped_images'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Add image info - use integer conversion for image_id\n",
        "        try:\n",
        "            img_id_int = int(image_id)\n",
        "        except ValueError:\n",
        "            # If image_id is not a valid integer, use hash\n",
        "            img_id_int = hash(image_id) % (10**8)\n",
        "            \n",
        "        image_info = {\n",
        "            \"id\": img_id_int,\n",
        "            \"file_name\": f\"{image_id}.jpg\",\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"license\": 1\n",
        "        }\n",
        "        coco_format[\"images\"].append(image_info)\n",
        "        conversion_stats['total_images'] += 1\n",
        "        \n",
        "        # Process annotations\n",
        "        xml_file = voc_path / 'Annotations' / f'{image_id}.xml'\n",
        "        if not xml_file.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            tree = ET.parse(xml_file)\n",
        "            root = tree.getroot()\n",
        "            \n",
        "            for obj in root.findall('object'):\n",
        "                class_name = obj.find('name').text\n",
        "                if class_name not in CLASS_TO_IDX:\n",
        "                    continue\n",
        "                \n",
        "                # Get bounding box\n",
        "                bbox_elem = obj.find('bndbox')\n",
        "                xmin = float(bbox_elem.find('xmin').text) - 1  # Convert to 0-based\n",
        "                ymin = float(bbox_elem.find('ymin').text) - 1\n",
        "                xmax = float(bbox_elem.find('xmax').text)\n",
        "                ymax = float(bbox_elem.find('ymax').text)\n",
        "                \n",
        "                # Convert to COCO format (x, y, width, height)\n",
        "                bbox_width = xmax - xmin\n",
        "                bbox_height = ymax - ymin\n",
        "                area = bbox_width * bbox_height\n",
        "                \n",
        "                # Add annotation\n",
        "                annotation = {\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": img_id_int,\n",
        "                    \"category_id\": CLASS_TO_IDX[class_name] + 1,  # COCO categories start from 1\n",
        "                    \"bbox\": [xmin, ymin, bbox_width, bbox_height],\n",
        "                    \"area\": area,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                coco_format[\"annotations\"].append(annotation)\n",
        "                annotation_id += 1\n",
        "                conversion_stats['total_annotations'] += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing annotations for {image_id}: {e}\")\n",
        "    \n",
        "    # Save COCO format JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_format, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"VOC to COCO conversion completed!\")\n",
        "    logger.info(f\"Statistics: {conversion_stats}\")\n",
        "    logger.info(f\"COCO file saved to: {output_file}\")\n",
        "    \n",
        "    return conversion_stats\n",
        "\n",
        "print(\"‚úÖ VOC to COCO conversion functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset and Transform Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Compose(object):\n",
        "    \"\"\"Compose transforms that take (image, target) and return (image, target).\"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert PIL image to Tensor (leave target untouched).\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        return F.to_tensor(image), target\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip image & boxes with probability p.\"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.p:\n",
        "            # flip image\n",
        "            image = F.hflip(image)\n",
        "            # flip boxes\n",
        "            w, _ = image.size\n",
        "            boxes = target[\"boxes\"].clone()\n",
        "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "    transforms.append(ToTensor())\n",
        "    return Compose(transforms)\n",
        "\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Pascal VOC Dataset for Faster R-CNN\"\"\"\n",
        "    def __init__(self, root, image_set=\"train\", transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # build list of image IDs\n",
        "        id_file = os.path.join(root, \"ImageSets\", \"Main\", f\"{image_set}.txt\")\n",
        "        with open(id_file) as f:\n",
        "            self.ids = [line.strip() for line in f]\n",
        "        \n",
        "        logger.info(f\"VOC Dataset initialized with {len(self.ids)} images for {image_set} set\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        # load image\n",
        "        img_path = os.path.join(self.root, \"JPEGImages\", f\"{img_id}.jpg\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # parse annotation\n",
        "        anno_path = os.path.join(self.root, \"Annotations\", f\"{img_id}.xml\")\n",
        "        tree = ET.parse(anno_path)\n",
        "        root = tree.getroot()\n",
        "        boxes, labels = [], []\n",
        "        for obj in root.findall(\"object\"):\n",
        "            cls = obj.find(\"name\").text\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            box = [int(bbox.find(x).text) for x in (\"xmin\",\"ymin\",\"xmax\",\"ymax\")]\n",
        "            boxes.append(box)\n",
        "            labels.append(VOC_CLASSES.index(cls) + 1)  # +1 because 0 is reserved for background\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        # other required fields\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "# Custom collate function for object detection\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for object detection datasets\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "print(\"‚úÖ Dataset and transform classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Setup and Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_fasterrcnn_model(num_classes=20, pretrained=True):\n",
        "    \"\"\"\n",
        "    Create Faster R-CNN model with ResNet50 backbone\n",
        "    \"\"\"\n",
        "    logger.info(f\"Creating Faster R-CNN model with {num_classes} classes (pretrained={pretrained})\")\n",
        "    \n",
        "    # Load pretrained Faster R-CNN model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
        "    \n",
        "    # Replace the classifier head for VOC classes\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)  # +1 for background\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    logger.info(f\"Model created successfully\")\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    training_metrics['total_parameters'] = total_params\n",
        "    \n",
        "    return model\n",
        "\n",
        "def save_checkpoint(model, optimizer, lr_scheduler, epoch, loss, filepath, is_best=False):\n",
        "    \"\"\"\n",
        "    Save model checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "        'training_metrics': training_metrics,\n",
        "        'config': CONFIG\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, filepath)\n",
        "    logger.info(f\"Checkpoint saved to {filepath}\")\n",
        "    \n",
        "    if is_best:\n",
        "        best_path = filepath.replace('.pth', '_best.pth')\n",
        "        torch.save(checkpoint, best_path)\n",
        "        logger.info(f\"Best model saved to {best_path}\")\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100):\n",
        "    \"\"\"\n",
        "    Train model for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(data_loader)\n",
        "    \n",
        "    logger.info(f\"Starting epoch {epoch + 1} training...\")\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        # Move data to device\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
        "                   for k, v in target.items()} for target in targets]\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        \n",
        "        # Calculate total loss\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Backward pass\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        \n",
        "        # Log progress\n",
        "        if (batch_idx + 1) % print_freq == 0 or batch_idx == num_batches - 1:\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            logger.info(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}, \"\n",
        "                       f\"Batch {batch_idx + 1}/{num_batches}, \"\n",
        "                       f\"Loss: {losses.item():.4f}, \"\n",
        "                       f\"Avg Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = total_loss / num_batches\n",
        "    \n",
        "    logger.info(f\"Epoch {epoch + 1} completed in {epoch_time:.1f}s, \"\n",
        "               f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
        "    \n",
        "    training_metrics['epoch_times'].append(epoch_time)\n",
        "    training_metrics['train_losses'].append(avg_epoch_loss)\n",
        "    \n",
        "    return avg_epoch_loss\n",
        "\n",
        "print(\"‚úÖ Model and training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device, coco_gt):\n",
        "    \"\"\"\n",
        "    Evaluate model using COCO metrics\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting model evaluation...\")\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "            if batch_idx % 50 == 0:\n",
        "                logger.info(f\"Evaluation batch {batch_idx + 1}/{len(dataloader)}\")\n",
        "            \n",
        "            images = [img.to(device) for img in images]\n",
        "            \n",
        "            # Get predictions\n",
        "            predictions = model(images)\n",
        "            \n",
        "            # Process each image in the batch\n",
        "            for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
        "                img_id = target['image_id'].item()\n",
        "                \n",
        "                boxes = pred['boxes'].cpu().numpy()\n",
        "                scores = pred['scores'].cpu().numpy()\n",
        "                labels = pred['labels'].cpu().numpy()\n",
        "                \n",
        "                # Filter by confidence threshold\n",
        "                keep = scores >= CONFIG['conf_threshold']\n",
        "                boxes = boxes[keep]\n",
        "                scores = scores[keep]\n",
        "                labels = labels[keep]\n",
        "                \n",
        "                # Convert to COCO format\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    all_predictions.append({\n",
        "                        \"image_id\": img_id,\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "    \n",
        "    if not all_predictions:\n",
        "        logger.warning(\"No predictions generated!\")\n",
        "        return {}\n",
        "    \n",
        "    # Save predictions\n",
        "    pred_file = f\"{CONFIG['output_dir']}/predictions/fasterrcnn_predictions.json\"\n",
        "    with open(pred_file, 'w') as f:\n",
        "        json.dump(all_predictions, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"Generated {len(all_predictions)} predictions\")\n",
        "    logger.info(f\"Predictions saved to {pred_file}\")\n",
        "    \n",
        "    # Evaluate with COCO metrics\n",
        "    try:\n",
        "        coco_pred = coco_gt.loadRes(pred_file)\n",
        "        coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
        "        coco_eval.params.imgIds = coco_gt.getImgIds()\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        coco_eval.summarize()\n",
        "        \n",
        "        # Extract metrics\n",
        "        metrics = {\n",
        "            'mAP_0.5_0.95': coco_eval.stats[0],\n",
        "            'mAP_0.5': coco_eval.stats[1],\n",
        "            'mAP_0.75': coco_eval.stats[2],\n",
        "            'mAP_small': coco_eval.stats[3],\n",
        "            'mAP_medium': coco_eval.stats[4],\n",
        "            'mAP_large': coco_eval.stats[5],\n",
        "            'AR_1': coco_eval.stats[6],\n",
        "            'AR_10': coco_eval.stats[7],\n",
        "            'AR_100': coco_eval.stats[8],\n",
        "            'AR_small': coco_eval.stats[9],\n",
        "            'AR_medium': coco_eval.stats[10],\n",
        "            'AR_large': coco_eval.stats[11]\n",
        "        }\n",
        "        \n",
        "        logger.info(\"COCO Evaluation Results:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            logger.info(f\"  {metric_name}: {value:.4f}\")\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"COCO evaluation failed: {e}\")\n",
        "        return {}\n",
        "\n",
        "def benchmark_inference_speed(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Benchmark model inference speed\n",
        "    \"\"\"\n",
        "    logger.info(\"Benchmarking inference speed...\")\n",
        "    model.eval()\n",
        "    \n",
        "    total_images = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            _ = model(images)\n",
        "            total_images += len(images)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    fps = total_images / elapsed_time\n",
        "    \n",
        "    logger.info(f\"Inference speed: {fps:.1f} FPS\")\n",
        "    return fps\n",
        "\n",
        "print(\"‚úÖ Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_training_progress(training_metrics, save_path=None):\n",
        "    \"\"\"\n",
        "    Create comprehensive training progress visualization\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f'Faster R-CNN Training Progress - {CONFIG[\"experiment_name\"]}', fontsize=16)\n",
        "    \n",
        "    # Training loss\n",
        "    if training_metrics['train_losses']:\n",
        "        axes[0, 0].plot(training_metrics['train_losses'], 'b-', linewidth=2)\n",
        "        axes[0, 0].set_title('Training Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Epoch times\n",
        "    if training_metrics['epoch_times']:\n",
        "        axes[0, 1].plot(training_metrics['epoch_times'], 'g-', linewidth=2)\n",
        "        axes[0, 1].set_title('Training Time per Epoch')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Time (seconds)')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning rates\n",
        "    if training_metrics['learning_rates']:\n",
        "        axes[1, 0].plot(training_metrics['learning_rates'], 'r-', linewidth=2)\n",
        "        axes[1, 0].set_title('Learning Rate Schedule')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Learning Rate')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_yscale('log')\n",
        "    \n",
        "    # Summary statistics\n",
        "    axes[1, 1].axis('off')\n",
        "    summary_text = f\"\"\"\n",
        "Training Summary:\n",
        "‚Ä¢ Total Parameters: {training_metrics['total_parameters']:,}\n",
        "‚Ä¢ Epochs Completed: {len(training_metrics['train_losses'])}\n",
        "‚Ä¢ Final Loss: {training_metrics['train_losses'][-1]:.4f if training_metrics['train_losses'] else 'N/A'}\n",
        "‚Ä¢ Best mAP@0.5: {training_metrics['best_map']:.4f}\n",
        "‚Ä¢ Avg Epoch Time: {np.mean(training_metrics['epoch_times']):.1f}s\n",
        "‚Ä¢ Total Training Time: {sum(training_metrics['epoch_times']):.1f}s\n",
        "\"\"\"\n",
        "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        logger.info(f\"Training progress plot saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions(model, dataset, device, num_images=5, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions on sample images\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Select random images\n",
        "    indices = random.sample(range(len(dataset)), min(num_images, len(dataset)))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, len(indices), figsize=(4 * len(indices), 8))\n",
        "    if len(indices) == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, target = dataset[idx]\n",
        "            \n",
        "            # Get prediction\n",
        "            model_input = image.unsqueeze(0).to(device)\n",
        "            prediction = model(model_input)[0]\n",
        "            \n",
        "            # Convert image back to numpy for visualization\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                img_array = image.permute(1, 2, 0).numpy()\n",
        "                img_array = np.clip(img_array, 0, 1)\n",
        "            else:\n",
        "                img_array = np.array(image) / 255.0\n",
        "            \n",
        "            # Ground truth visualization\n",
        "            axes[0, i].imshow(img_array)\n",
        "            axes[0, i].set_title(f'Ground Truth')\n",
        "            axes[0, i].axis('off')\n",
        "            \n",
        "            # Draw ground truth boxes\n",
        "            if len(target['boxes']) > 0:\n",
        "                for box, label in zip(target['boxes'], target['labels']):\n",
        "                    x1, y1, x2, y2 = box.numpy()\n",
        "                    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                           linewidth=2, edgecolor='green', facecolor='none')\n",
        "                    axes[0, i].add_patch(rect)\n",
        "                    axes[0, i].text(x1, y1-5, VOC_CLASSES[label.item()-1], \n",
        "                                   color='green', fontsize=8, weight='bold')\n",
        "            \n",
        "            # Prediction visualization\n",
        "            axes[1, i].imshow(img_array)\n",
        "            axes[1, i].set_title(f'Predictions')\n",
        "            axes[1, i].axis('off')\n",
        "            \n",
        "            # Draw prediction boxes\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "            \n",
        "            # Filter by confidence\n",
        "            keep = scores >= CONFIG['conf_threshold']\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            \n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
        "                axes[1, i].add_patch(rect)\n",
        "                axes[1, i].text(x1, y1-5, f'{VOC_CLASSES[label-1]} ({score:.2f})', \n",
        "                               color='red', fontsize=8, weight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        logger.info(f\"Prediction visualization saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def visualize_dataset_samples(dataset, num_samples=3):\n",
        "    \"\"\"\n",
        "    Visualize sample images from dataset\n",
        "    \"\"\"\n",
        "    for idx in random.sample(range(len(dataset)), num_samples):\n",
        "        img, target = dataset[idx]\n",
        "        # handle both Tensor and PIL\n",
        "        if torch.is_tensor(img):\n",
        "            img_np = img.mul(255).permute(1,2,0).byte().cpu().numpy()\n",
        "        else:\n",
        "            img_np = np.array(img)\n",
        "        boxes  = target[\"boxes\"].cpu().numpy()\n",
        "        labels = target[\"labels\"].cpu().numpy()\n",
        "\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.imshow(img_np)\n",
        "        ax = plt.gca()\n",
        "        for box, label in zip(boxes, labels):\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            ax.add_patch(plt.Rectangle((xmin,ymin), xmax-xmin, ymax-ymin,\n",
        "                                       fill=False, edgecolor=\"g\", linewidth=2))\n",
        "            ax.text(xmin, ymin-2, VOC_CLASSES[label-1],\n",
        "                    fontsize=10, color=\"white\", backgroundcolor=\"g\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Comparison Images Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_comparison_images(model, dataset, device, comparison_file_path=None):\n",
        "    \"\"\"\n",
        "    Test model on comparison images and create side-by-side visualizations\n",
        "    \"\"\"\n",
        "    # Define comparison images if file not provided\n",
        "    if comparison_file_path and os.path.exists(comparison_file_path):\n",
        "        with open(comparison_file_path, 'r') as f:\n",
        "            comparison_data = json.load(f)\n",
        "        comparison_images = comparison_data.get('images', [])\n",
        "    else:\n",
        "        # Define some default comparison images\n",
        "        comparison_images = [\n",
        "            {\"image_id\": \"2008_000002\", \"filename\": \"2008_000002.jpg\", \n",
        "             \"description\": \"Person with horse\", \"expected_objects\": [\"person\", \"horse\"], \"difficulty\": \"medium\"},\n",
        "            {\"image_id\": \"2008_000008\", \"filename\": \"2008_000008.jpg\", \n",
        "             \"description\": \"Multiple cars\", \"expected_objects\": [\"car\"], \"difficulty\": \"easy\"},\n",
        "            {\"image_id\": \"2008_000015\", \"filename\": \"2008_000015.jpg\", \n",
        "             \"description\": \"Birds in scene\", \"expected_objects\": [\"bird\"], \"difficulty\": \"hard\"},\n",
        "            {\"image_id\": \"2008_000019\", \"filename\": \"2008_000019.jpg\", \n",
        "             \"description\": \"Person scene\", \"expected_objects\": [\"person\"], \"difficulty\": \"medium\"},\n",
        "            {\"image_id\": \"2008_000021\", \"filename\": \"2008_000021.jpg\", \n",
        "             \"description\": \"Multiple objects\", \"expected_objects\": [\"person\", \"bicycle\"], \"difficulty\": \"hard\"}\n",
        "        ]\n",
        "    \n",
        "    logger.info(f\"Testing on {len(comparison_images)} comparison images...\")\n",
        "    \n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    # Find comparison images in dataset\n",
        "    dataset_filenames = {}\n",
        "    for idx in range(len(dataset)):\n",
        "        img_id = dataset.ids[idx]\n",
        "        filename = f\"{img_id}.jpg\"\n",
        "        dataset_filenames[filename] = idx\n",
        "    \n",
        "    comparison_found = []\n",
        "    for img_data in comparison_images:\n",
        "        if img_data['filename'] in dataset_filenames:\n",
        "            comparison_found.append((img_data, dataset_filenames[img_data['filename']]))\n",
        "    \n",
        "    if not comparison_found:\n",
        "        logger.warning(\"No comparison images found in dataset!\")\n",
        "        return []\n",
        "    \n",
        "    logger.info(f\"Found {len(comparison_found)} comparison images in dataset\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(len(comparison_found), 2, figsize=(12, 4 * len(comparison_found)))\n",
        "    if len(comparison_found) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for row, (img_data, idx) in enumerate(comparison_found):\n",
        "            image, target = dataset[idx]\n",
        "            \n",
        "            # Get prediction\n",
        "            model_input = image.unsqueeze(0).to(device)\n",
        "            prediction = model(model_input)[0]\n",
        "            \n",
        "            # Convert image for visualization\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                img_array = image.permute(1, 2, 0).numpy()\n",
        "                img_array = np.clip(img_array, 0, 1)\n",
        "            else:\n",
        "                img_array = np.array(image) / 255.0\n",
        "            \n",
        "            # Ground truth (left side)\n",
        "            axes[row, 0].imshow(img_array)\n",
        "            axes[row, 0].set_title(f'Ground Truth - {img_data[\"description\"]}\\nExpected: {\", \".join(img_data[\"expected_objects\"])}')\n",
        "            axes[row, 0].axis('off')\n",
        "            \n",
        "            # Draw ground truth boxes\n",
        "            gt_objects = []\n",
        "            if len(target['boxes']) > 0:\n",
        "                for box, label in zip(target['boxes'], target['labels']):\n",
        "                    x1, y1, x2, y2 = box.numpy()\n",
        "                    class_name = VOC_CLASSES[label.item()-1]\n",
        "                    gt_objects.append(class_name)\n",
        "                    \n",
        "                    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                           linewidth=2, edgecolor='green', facecolor='none')\n",
        "                    axes[row, 0].add_patch(rect)\n",
        "                    axes[row, 0].text(x1, y1-5, class_name, \n",
        "                                     color='green', fontsize=10, weight='bold',\n",
        "                                     bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
        "            \n",
        "            # Predictions (right side)\n",
        "            axes[row, 1].imshow(img_array)\n",
        "            axes[row, 1].set_title(f'Predictions - Difficulty: {img_data[\"difficulty\"]}')\n",
        "            axes[row, 1].axis('off')\n",
        "            \n",
        "            # Draw prediction boxes\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "            \n",
        "            # Filter by confidence\n",
        "            keep = scores >= CONFIG['conf_threshold']\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            \n",
        "            pred_objects = []\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "                class_name = VOC_CLASSES[label-1]\n",
        "                pred_objects.append(class_name)\n",
        "                \n",
        "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
        "                axes[row, 1].add_patch(rect)\n",
        "                axes[row, 1].text(x1, y1-5, f'{class_name}\\n{score:.2f}', \n",
        "                                 color='red', fontsize=10, weight='bold',\n",
        "                                 bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
        "            \n",
        "            # Store results\n",
        "            result = {\n",
        "                'image_id': img_data['image_id'],\n",
        "                'filename': img_data['filename'],\n",
        "                'description': img_data['description'],\n",
        "                'difficulty': img_data['difficulty'],\n",
        "                'expected_objects': img_data['expected_objects'],\n",
        "                'ground_truth_objects': gt_objects,\n",
        "                'predicted_objects': pred_objects,\n",
        "                'num_predictions': len(pred_objects),\n",
        "                'avg_confidence': float(np.mean(scores)) if len(scores) > 0 else 0.0\n",
        "            }\n",
        "            results.append(result)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save comparison visualization\n",
        "    comparison_viz_path = f\"{CONFIG['output_dir']}/visualizations/comparison_predictions.png\"\n",
        "    plt.savefig(comparison_viz_path, dpi=300, bbox_inches='tight')\n",
        "    logger.info(f\"Comparison visualization saved to {comparison_viz_path}\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Save results\n",
        "    results_path = f\"{CONFIG['output_dir']}/predictions/comparison_results.json\"\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"Comparison results saved to {results_path}\")\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\nüéØ Comparison Images Results Summary:\")\n",
        "    for result in results:\n",
        "        expected = set(result['expected_objects'])\n",
        "        predicted = set(result['predicted_objects'])\n",
        "        overlap = expected.intersection(predicted)\n",
        "        \n",
        "        print(f\"\\nüì∑ {result['filename']} ({result['difficulty']})\")\n",
        "        print(f\"   Expected: {', '.join(expected)}\")\n",
        "        print(f\"   Predicted: {', '.join(predicted)}\")\n",
        "        print(f\"   Overlap: {', '.join(overlap) if overlap else 'None'}\")\n",
        "        print(f\"   Avg Confidence: {result['avg_confidence']:.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Comparison testing functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Data Preparation - Convert VOC to COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert VOC dataset to COCO format\n",
        "logger.info(\"Starting data preparation...\")\n",
        "\n",
        "coco_file = f\"{CONFIG['output_dir']}/data/voc2012_coco.json\"\n",
        "\n",
        "print(f\"üîÑ Converting VOC dataset to COCO format...\")\n",
        "print(f\"   VOC root: {CONFIG['voc_root']}\")\n",
        "print(f\"   Output file: {coco_file}\")\n",
        "\n",
        "# Check if VOC dataset exists\n",
        "if not os.path.exists(CONFIG['voc_root']):\n",
        "    logger.error(f\"VOC dataset not found at {CONFIG['voc_root']}\")\n",
        "    print(f\"‚ùå VOC dataset not found at {CONFIG['voc_root']}\")\n",
        "    print(\"Please update the 'voc_root' path in the configuration section.\")\n",
        "else:\n",
        "    # Convert dataset\n",
        "    conversion_stats = convert_voc_to_coco(CONFIG['voc_root'], coco_file, 'trainval')\n",
        "    \n",
        "    if conversion_stats:\n",
        "        print(f\"‚úÖ Conversion completed successfully!\")\n",
        "        print(f\"   Total images: {conversion_stats['total_images']}\")\n",
        "        print(f\"   Total annotations: {conversion_stats['total_annotations']}\")\n",
        "        print(f\"   Skipped images: {conversion_stats['skipped_images']}\")\n",
        "        \n",
        "        CONFIG['coco_file'] = coco_file\n",
        "        CONFIG['image_dir'] = f\"{CONFIG['voc_root']}/JPEGImages\"\n",
        "    else:\n",
        "        print(f\"‚ùå Conversion failed!\")\n",
        "        logger.error(\"VOC to COCO conversion failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Dataset and DataLoader Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "logger.info(\"Creating datasets and dataloaders...\")\n",
        "\n",
        "try:\n",
        "    # Create full datasets\n",
        "    dataset_train = VOCDataset(CONFIG['voc_root'], \"train\", transforms=get_transform(True))\n",
        "    dataset_val = VOCDataset(CONFIG['voc_root'], \"val\", transforms=get_transform(False))\n",
        "    \n",
        "    # For quick testing, you can use a subset (uncomment the lines below)\n",
        "    # indices = torch.randperm(len(dataset_train)).tolist()\n",
        "    # dataset_train = Subset(dataset_train, indices[:200])\n",
        "    # dataset_val = Subset(dataset_val, indices[:50])\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=CONFIG['num_workers'],\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if CONFIG['device'] == 'cuda' else False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        dataset_val,\n",
        "        batch_size=1,  # Use batch size 1 for evaluation\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if CONFIG['device'] == 'cuda' else False\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Datasets created successfully!\")\n",
        "    print(f\"   Training samples: {len(dataset_train)}\")\n",
        "    print(f\"   Validation samples: {len(dataset_val)}\")\n",
        "    print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Validation batches: {len(val_loader)}\")\n",
        "    \n",
        "    logger.info(f\"Datasets created - Train: {len(dataset_train)}, Val: {len(dataset_val)}\")\n",
        "    logger.info(f\"DataLoaders created - Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create datasets: {e}\")\n",
        "    print(f\"‚ùå Failed to create datasets: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dataset samples\n",
        "print(\"üì∏ Visualizing dataset samples...\")\n",
        "try:\n",
        "    visualize_dataset_samples(dataset_train, num_samples=3)\n",
        "    logger.info(\"Dataset visualization completed\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Dataset visualization failed: {e}\")\n",
        "    print(f\"‚ö†Ô∏è Dataset visualization failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "logger.info(\"Creating Faster R-CNN model...\")\n",
        "\n",
        "try:\n",
        "    model = create_fasterrcnn_model(num_classes=len(VOC_CLASSES), pretrained=True)\n",
        "    model = model.to(CONFIG['device'])\n",
        "    \n",
        "    # Setup optimizer and scheduler\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=CONFIG['learning_rate'],\n",
        "        momentum=CONFIG['momentum'],\n",
        "        weight_decay=CONFIG['weight_decay']\n",
        "    )\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "    \n",
        "    # Calculate model size\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "    training_metrics['model_size_mb'] = model_size\n",
        "    \n",
        "    print(f\"‚úÖ Model created successfully!\")\n",
        "    print(f\"   Model: Faster R-CNN with ResNet50 backbone\")\n",
        "    print(f\"   Device: {CONFIG['device']}\")\n",
        "    print(f\"   Parameters: {training_metrics['total_parameters']:,}\")\n",
        "    print(f\"   Model size: {model_size:.1f} MB\")\n",
        "    print(f\"   Optimizer: SGD (lr={CONFIG['learning_rate']}, momentum={CONFIG['momentum']})\")\n",
        "    \n",
        "    logger.info(f\"Model setup completed - Device: {CONFIG['device']}, Size: {model_size:.1f}MB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create model: {e}\")\n",
        "    print(f\"‚ùå Failed to create model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with enhanced monitoring\n",
        "logger.info(\"Starting training...\")\n",
        "training_metrics['start_time'] = datetime.now()\n",
        "\n",
        "print(f\"üöÄ Starting Faster R-CNN training...\")\n",
        "print(f\"   Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"   Device: {CONFIG['device']}\")\n",
        "\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Check for existing checkpoint\n",
        "ckpt_path = f\"{CONFIG['output_dir']}/models/checkpoint_latest.pth\"\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    try:\n",
        "        ckpt = torch.load(ckpt_path, map_location=CONFIG['device'])\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        lr_scheduler.load_state_dict(ckpt['lr_scheduler_state_dict'])\n",
        "        start_epoch = ckpt['epoch']\n",
        "        best_loss = ckpt.get('loss', float('inf'))\n",
        "        print(f\"üìÇ Resuming from epoch {start_epoch + 1}\")\n",
        "        logger.info(f\"Resuming training from epoch {start_epoch + 1}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to load checkpoint: {e}\")\n",
        "        print(f\"‚ö†Ô∏è Failed to load checkpoint, starting from scratch\")\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
        "        logger.info(f\"Starting epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "        print(f\"\\nüìä Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "        \n",
        "        # Train for one epoch\n",
        "        epoch_loss = train_one_epoch(model, optimizer, train_loader, CONFIG['device'], epoch)\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Update learning rate tracking\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        training_metrics['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Save checkpoint\n",
        "        if CONFIG['save_checkpoints'] and (epoch + 1) % CONFIG['checkpoint_every'] == 0:\n",
        "            checkpoint_path = f\"{CONFIG['output_dir']}/models/fasterrcnn_epoch_{epoch + 1}.pth\"\n",
        "            is_best = epoch_loss < best_loss\n",
        "            if is_best:\n",
        "                best_loss = epoch_loss\n",
        "            \n",
        "            save_checkpoint(model, optimizer, lr_scheduler, epoch + 1, epoch_loss, checkpoint_path, is_best)\n",
        "            \n",
        "            # Also save as latest checkpoint\n",
        "            save_checkpoint(model, optimizer, lr_scheduler, epoch + 1, epoch_loss, ckpt_path)\n",
        "        \n",
        "        # Print epoch summary\n",
        "        print(f\"   ‚úÖ Epoch {epoch + 1} completed\")\n",
        "        print(f\"   üìâ Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"   ‚è±Ô∏è  Time: {training_metrics['epoch_times'][-1]:.1f}s\")\n",
        "        print(f\"   üìö Learning Rate: {current_lr:.6f}\")\n",
        "        \n",
        "        # Early visualization of training progress\n",
        "        if (epoch + 1) % 2 == 0 or epoch == CONFIG['num_epochs'] - 1:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Training Progress Update - Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "            if len(training_metrics['train_losses']) > 1:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.plot(training_metrics['train_losses'], 'b-', linewidth=2)\n",
        "                plt.title('Training Loss')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.plot(training_metrics['epoch_times'], 'g-', linewidth=2)\n",
        "                plt.title('Epoch Time')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Time (s)')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "    \n",
        "    training_metrics['end_time'] = datetime.now()\n",
        "    total_training_time = training_metrics['end_time'] - training_metrics['start_time']\n",
        "    \n",
        "    print(f\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"   Total time: {total_training_time}\")\n",
        "    print(f\"   Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "    print(f\"   Best loss: {best_loss:.4f}\")\n",
        "    \n",
        "    logger.info(f\"Training completed successfully in {total_training_time}\")\n",
        "    logger.info(f\"Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    training_metrics['end_time'] = datetime.now()\n",
        "    logger.error(f\"Training failed: {e}\")\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "logger.info(\"Starting model evaluation...\")\n",
        "\n",
        "print(f\"üîç Evaluating trained model...\")\n",
        "\n",
        "try:\n",
        "    # Load ground truth COCO for evaluation\n",
        "    coco_gt = COCO(CONFIG['coco_file'])\n",
        "    \n",
        "    # Fix missing fields in COCO GT (required by pycocotools)\n",
        "    if 'info' not in coco_gt.dataset:\n",
        "        coco_gt.dataset['info'] = {}\n",
        "    if 'licenses' not in coco_gt.dataset:\n",
        "        coco_gt.dataset['licenses'] = []\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    eval_metrics = evaluate_model(model, val_loader, CONFIG['device'], coco_gt)\n",
        "    \n",
        "    if eval_metrics:\n",
        "        training_metrics['final_metrics'] = eval_metrics\n",
        "        training_metrics['best_map'] = eval_metrics.get('mAP_0.5', 0.0)\n",
        "        \n",
        "        print(f\"\\nüìä Evaluation Results:\")\n",
        "        print(f\"   mAP@0.5-0.95: {eval_metrics.get('mAP_0.5_0.95', 0):.4f}\")\n",
        "        print(f\"   mAP@0.5: {eval_metrics.get('mAP_0.5', 0):.4f}\")\n",
        "        print(f\"   mAP@0.75: {eval_metrics.get('mAP_0.75', 0):.4f}\")\n",
        "        print(f\"   AR@100: {eval_metrics.get('AR_100', 0):.4f}\")\n",
        "        \n",
        "        logger.info(f\"Evaluation completed - mAP@0.5: {eval_metrics.get('mAP_0.5', 0):.4f}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Evaluation completed but no metrics generated\")\n",
        "        logger.warning(\"Evaluation completed but no metrics generated\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logger.error(f\"Evaluation failed: {e}\")\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Inference Speed Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark inference speed\n",
        "logger.info(\"Benchmarking inference speed...\")\n",
        "\n",
        "try:\n",
        "    fps = benchmark_inference_speed(model, val_loader, CONFIG['device'])\n",
        "    \n",
        "    # Save metrics including FPS\n",
        "    all_metrics = {\n",
        "        \"mAP_0.5_0.95\": training_metrics['final_metrics'].get('mAP_0.5_0.95', 0.0),\n",
        "        \"mAP_0.5\": training_metrics['final_metrics'].get('mAP_0.5', 0.0),\n",
        "        \"mAP_0.75\": training_metrics['final_metrics'].get('mAP_0.75', 0.0),\n",
        "        \"FPS\": fps\n",
        "    }\n",
        "    \n",
        "    metrics_file = f\"{CONFIG['output_dir']}/predictions/fasterrcnn_metrics.json\"\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(all_metrics, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n‚ö° Performance Summary:\")\n",
        "    print(f\"   Inference Speed: {fps:.1f} FPS\")\n",
        "    print(f\"   Metrics saved to: {metrics_file}\")\n",
        "    \n",
        "    logger.info(f\"Speed benchmarking completed - FPS: {fps:.1f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Speed benchmarking failed: {e}\")\n",
        "    print(f\"‚ö†Ô∏è Speed benchmarking failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Training Progress Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training visualization\n",
        "if CONFIG['save_visualizations']:\n",
        "    logger.info(\"Creating training progress visualization...\")\n",
        "    \n",
        "    viz_path = f\"{CONFIG['output_dir']}/visualizations/training_progress.png\"\n",
        "    visualize_training_progress(training_metrics, viz_path)\n",
        "    \n",
        "    print(f\"‚úÖ Training visualization saved to {viz_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Sample Predictions Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample predictions\n",
        "if CONFIG['save_visualizations']:\n",
        "    logger.info(\"Creating sample predictions visualization...\")\n",
        "    \n",
        "    pred_viz_path = f\"{CONFIG['output_dir']}/visualizations/sample_predictions.png\"\n",
        "    visualize_predictions(model, dataset_val, CONFIG['device'], num_images=4, save_path=pred_viz_path)\n",
        "    \n",
        "    print(f\"‚úÖ Sample predictions visualization saved to {pred_viz_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Comparison Images Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on comparison images\n",
        "if CONFIG['run_comparison_images']:\n",
        "    logger.info(\"Testing on comparison images...\")\n",
        "    \n",
        "    print(f\"üñºÔ∏è  Testing on comparison images...\")\n",
        "    \n",
        "    # Try to find comparison_images.json in common locations\n",
        "    comparison_file_paths = [\n",
        "        '/kaggle/input/comparison-images/comparison_images.json',\n",
        "        '/kaggle/working/comparison_images.json',\n",
        "        './comparison_images.json',\n",
        "        '../comparison_images.json'\n",
        "    ]\n",
        "    \n",
        "    comparison_file = None\n",
        "    for path in comparison_file_paths:\n",
        "        if os.path.exists(path):\n",
        "            comparison_file = path\n",
        "            break\n",
        "    \n",
        "    if comparison_file:\n",
        "        print(f\"üìÅ Found comparison file: {comparison_file}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No comparison_images.json found, using default images\")\n",
        "    \n",
        "    try:\n",
        "        comparison_results = test_comparison_images(model, dataset_val, CONFIG['device'], comparison_file)\n",
        "        \n",
        "        print(f\"\\n‚úÖ Comparison testing completed!\")\n",
        "        print(f\"   Tested images: {len(comparison_results)}\")\n",
        "        \n",
        "        logger.info(f\"Comparison testing completed on {len(comparison_results)} images\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Comparison testing failed: {e}\")\n",
        "        print(f\"‚ùå Comparison testing failed: {e}\")\n",
        "else:\n",
        "    print(f\"‚è≠Ô∏è  Skipping comparison images testing (disabled in config)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 20. Final Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive final report\n",
        "logger.info(\"Generating final report...\")\n",
        "\n",
        "final_report = {\n",
        "    'experiment_info': {\n",
        "        'name': CONFIG['experiment_name'],\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model': 'Faster R-CNN',\n",
        "        'backbone': 'ResNet50 + FPN',\n",
        "        'dataset': 'Pascal VOC 2012',\n",
        "        'device': CONFIG['device']\n",
        "    },\n",
        "    'configuration': CONFIG,\n",
        "    'training_metrics': training_metrics,\n",
        "    'dataset_info': {\n",
        "        'train_samples': len(dataset_train) if 'dataset_train' in locals() else 0,\n",
        "        'val_samples': len(dataset_val) if 'dataset_val' in locals() else 0,\n",
        "        'num_classes': len(VOC_CLASSES),\n",
        "        'classes': VOC_CLASSES\n",
        "    },\n",
        "    'model_info': {\n",
        "        'total_parameters': training_metrics['total_parameters'],\n",
        "        'model_size_mb': training_metrics['model_size_mb'],\n",
        "        'optimizer': 'SGD',\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'backbone': 'ResNet50 with Feature Pyramid Network'\n",
        "    },\n",
        "    'output_files': {\n",
        "        'log_file': f\"{CONFIG['output_dir']}/logs/{CONFIG['experiment_name']}.log\",\n",
        "        'final_model': f\"{CONFIG['output_dir']}/models/fasterrcnn_epoch_{CONFIG['num_epochs']}.pth\",\n",
        "        'best_model': f\"{CONFIG['output_dir']}/models/fasterrcnn_epoch_{CONFIG['num_epochs']}_best.pth\",\n",
        "        'predictions': f\"{CONFIG['output_dir']}/predictions/fasterrcnn_predictions.json\",\n",
        "        'comparison_results': f\"{CONFIG['output_dir']}/predictions/comparison_results.json\",\n",
        "        'metrics': f\"{CONFIG['output_dir']}/predictions/fasterrcnn_metrics.json\",\n",
        "        'training_viz': f\"{CONFIG['output_dir']}/visualizations/training_progress.png\",\n",
        "        'predictions_viz': f\"{CONFIG['output_dir']}/visualizations/sample_predictions.png\",\n",
        "        'comparison_viz': f\"{CONFIG['output_dir']}/visualizations/comparison_predictions.png\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save final report\n",
        "report_file = f\"{CONFIG['output_dir']}/{CONFIG['experiment_name']}_final_report.json\"\n",
        "with open(report_file, 'w') as f:\n",
        "    json.dump(final_report, f, indent=2, default=str)\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{CONFIG['output_dir']}/models/fasterrcnn_final.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'training_metrics': training_metrics,\n",
        "    'final_report': final_report\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"\\nüéâ Faster R-CNN Pipeline Completed Successfully!\")\n",
        "print(f\"\\nüìã Final Summary:\")\n",
        "print(f\"   Experiment: {CONFIG['experiment_name']}\")\n",
        "print(f\"   Device: {CONFIG['device']}\")\n",
        "print(f\"   Training time: {training_metrics['end_time'] - training_metrics['start_time']}\")\n",
        "print(f\"   Epochs completed: {len(training_metrics['train_losses'])}/{CONFIG['num_epochs']}\")\n",
        "print(f\"   Final loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
        "if training_metrics['best_map'] > 0:\n",
        "    print(f\"   Best mAP@0.5: {training_metrics['best_map']:.4f}\")\n",
        "print(f\"   Model size: {training_metrics['model_size_mb']:.1f} MB\")\n",
        "print(f\"   Parameters: {training_metrics['total_parameters']:,}\")\n",
        "\n",
        "print(f\"\\nüìÅ Output Files:\")\n",
        "print(f\"   üìä Final report: {report_file}\")\n",
        "print(f\"   ü§ñ Final model: {final_model_path}\")\n",
        "print(f\"   üìù Training log: {log_file}\")\n",
        "print(f\"   üìà Visualizations: {CONFIG['output_dir']}/visualizations/\")\n",
        "print(f\"   üéØ Predictions: {CONFIG['output_dir']}/predictions/\")\n",
        "\n",
        "logger.info(f\"Faster R-CNN pipeline completed successfully!\")\n",
        "logger.info(f\"Final report saved to {report_file}\")\n",
        "logger.info(f\"Final model saved to {final_model_path}\")\n",
        "logger.info(\"=== EXPERIMENT COMPLETED ===\")\n",
        "\n",
        "print(f\"\\n‚ú® All done! Check the output directory for all generated files.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}