{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Object Detection Training and Evaluation Pipeline\n",
    "\n",
    "This notebook demonstrates a complete pipeline for training and evaluating an SSD300 model on Pascal VOC 2012 dataset.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Preparation**: Convert VOC XML annotations to COCO format\n",
    "2. **Dataset Loading**: Create optimized dataset loaders\n",
    "3. **Model Training**: Train SSD300 with proper configuration\n",
    "4. **Model Evaluation**: Evaluate using COCO metrics\n",
    "5. **Performance Analysis**: Speed, memory, and accuracy benchmarking\n",
    "6. **Visualization**: View inference results\n",
    "\n",
    "## Requirements:\n",
    "- Pascal VOC 2012 dataset\n",
    "- PyTorch with torchvision\n",
    "- pycocotools\n",
    "- Standard ML libraries (numpy, matplotlib, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import ssd300_vgg16, SSD300_VGG16_Weights\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Pascal VOC classes\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation: VOC to COCO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels file for VOC classes\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "with open(\"data/labels.txt\", \"w\") as f:\n",
    "    for cls in VOC_CLASSES:\n",
    "        f.write(f\"{cls}\\n\")\n",
    "\n",
    "print(\"Created labels.txt with Pascal VOC classes\")\n",
    "print(f\"Classes: {', '.join(VOC_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert VOC XML annotations to COCO JSON format\n",
    "# Note: Update these paths according to your dataset location\n",
    "VOC_DATA_DIR = \"data/VOCdevkit/VOC2012\"\n",
    "ANNOTATIONS_DIR = f\"{VOC_DATA_DIR}/Annotations\"\n",
    "IMAGES_DIR = f\"{VOC_DATA_DIR}/JPEGImages\"\n",
    "TRAINVAL_IDS = f\"{VOC_DATA_DIR}/ImageSets/Main/trainval.txt\"\n",
    "COCO_JSON_PATH = \"data/voc2012_coco.json\"\n",
    "\n",
    "# Run VOC to COCO conversion using the existing script\n",
    "conversion_cmd = f\"\"\"\n",
    "python scripts/voc2coco.py \\\n",
    "  --ann_dir {ANNOTATIONS_DIR} \\\n",
    "  --ann_ids {TRAINVAL_IDS} \\\n",
    "  --labels data/labels.txt \\\n",
    "  --output {COCO_JSON_PATH} \\\n",
    "  --ext xml \\\n",
    "  --extract_num_from_imgid\n",
    "\"\"\"\n",
    "\n",
    "print(\"Converting VOC annotations to COCO format...\")\n",
    "print(f\"Command: {conversion_cmd.strip()}\")\n",
    "os.system(conversion_cmd)\n",
    "print(f\"Conversion complete! COCO JSON saved to: {COCO_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading with Optimized COCO Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized COCO dataset loader\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class COCODetectionDataset(Dataset):\n",
    "    \"\"\"Optimized COCO dataset loader for object detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, ann_file, img_folder, transform=None):\n",
    "        print(f\"Loading COCO annotations from: {ann_file}\")\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        print(f\"Loaded {len(self.ids)} images\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_folder, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'] - 1)  # Convert to 0-based indexing\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    \"\"\"Get transforms for SSD300 input preprocessing.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize((300, 300)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.48235, 0.45882, 0.40784], \n",
    "                   std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for object detection.\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "print(\"Creating dataset and dataloader...\")\n",
    "dataset = COCODetectionDataset(COCO_JSON_PATH, IMAGES_DIR, transform=get_transform())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "print(f\"Dataset ready with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SSD300 model\n",
    "print(\"Initializing SSD300 model...\")\n",
    "model = ssd300_vgg16(weights=SSD300_VGG16_Weights.COCO_V1)\n",
    "\n",
    "# Replace classification head for Pascal VOC (21 classes including background)\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=[512, 1024, 512, 256, 256, 256],\n",
    "    num_anchors=[4, 6, 6, 6, 4, 4],\n",
    "    num_classes=21\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "print(f\"Model loaded on {DEVICE}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_SAVE_PATH = \"outputs/ssd300_voc.pth\"\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {dataloader.batch_size}\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")\n",
    "print(f\"  Batches per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        batch_loss = losses.item()\n",
    "        epoch_loss += batch_loss\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'batch_loss': f'{batch_loss:.4f}',\n",
    "            'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "    \n",
    "    # Log epoch results\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    training_losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(f\"Training completed! Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, EPOCHS+1), training_losses, 'b-', marker='o', linewidth=2, markersize=8)\n",
    "plt.title('SSD300 Training Loss', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, EPOCHS+1))\n",
    "for i, loss in enumerate(training_losses):\n",
    "    plt.annotate(f'{loss:.3f}', (i+1, loss), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {((training_losses[0] - training_losses[-1]) / training_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation\n",
    "print(\"Setting up model for evaluation...\")\n",
    "model.eval()\n",
    "\n",
    "# Create evaluation dataloader (no shuffling)\n",
    "eval_dataloader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load ground truth COCO\n",
    "coco_gt = COCO(COCO_JSON_PATH)\n",
    "if 'info' not in coco_gt.dataset:\n",
    "    coco_gt.dataset['info'] = {\"description\": \"VOC to COCO converted dataset\"}\n",
    "\n",
    "print(\"Running inference on validation set...\")\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for output, target in zip(outputs, targets):\n",
    "            img_id = target['image_id'].item()\n",
    "            \n",
    "            for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
    "                x1, y1, x2, y2 = box.cpu().tolist()\n",
    "                all_predictions.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(label) + 1,  # Convert back to 1-based for COCO\n",
    "                    \"bbox\": [x1, y1, x2 - x1, y2 - y1],  # COCO format: [x, y, width, height]\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "print(f\"Generated {len(all_predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and run COCO evaluation\n",
    "RESULTS_PATH = \"outputs/ssd_results.json\"\n",
    "\n",
    "print(f\"Saving predictions to: {RESULTS_PATH}\")\n",
    "with open(RESULTS_PATH, 'w') as f:\n",
    "    json.dump(all_predictions, f)\n",
    "\n",
    "print(\"Running COCO evaluation...\")\n",
    "coco_dt = coco_gt.loadRes(RESULTS_PATH)\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "\n",
    "# Filter to only evaluate on images that have predictions\n",
    "img_ids_with_preds = sorted({pred['image_id'] for pred in all_predictions})\n",
    "coco_eval.params.imgIds = img_ids_with_preds\n",
    "\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save detailed metrics\n",
    "metrics_keys = [\n",
    "    \"AP_0.50:0.95\", \"AP_0.50\", \"AP_0.75\",\n",
    "    \"AP_small\", \"AP_medium\", \"AP_large\",\n",
    "    \"AR_1\", \"AR_10\", \"AR_100\",\n",
    "    \"AR_small\", \"AR_medium\", \"AR_large\"\n",
    "]\n",
    "\n",
    "metrics_dict = dict(zip(metrics_keys, coco_eval.stats.tolist()))\n",
    "\n",
    "# Add per-class Average Precision\n",
    "precision = coco_eval.eval['precision']  # [IoU, Recall, Category, Area, MaxDets]\n",
    "cat_ids = coco_eval.params.catIds\n",
    "cat_names = {cat['id']: cat['name'] for cat in coco_gt.loadCats(cat_ids)}\n",
    "\n",
    "per_class_ap = {}\n",
    "print(\"\\nPer-class Average Precision (AP @ IoU=0.50:0.95):\")\n",
    "print(\"=\" * 50)\n",
    "for i, cat_id in enumerate(cat_ids):\n",
    "    # Extract precision for this category (all IoUs, all recalls, area=all, maxDets=100)\n",
    "    cat_precision = precision[:, :, i, 0, 2]\n",
    "    # Average precision is mean of valid precision values\n",
    "    ap = np.mean(cat_precision[cat_precision > -1])\n",
    "    per_class_ap[cat_names[cat_id]] = float(ap)\n",
    "    print(f\"{cat_names[cat_id]:15s}: {ap:.3f}\")\n",
    "\n",
    "metrics_dict['per_class_AP'] = per_class_ap\n",
    "\n",
    "# Save comprehensive metrics\n",
    "METRICS_PATH = \"outputs/evaluation_metrics.json\"\n",
    "with open(METRICS_PATH, 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nDetailed metrics saved to: {METRICS_PATH}\")\n",
    "print(f\"\\nKey Results:\")\n",
    "print(f\"  mAP @ IoU=0.50:0.95: {metrics_dict['AP_0.50:0.95']:.3f}\")\n",
    "print(f\"  mAP @ IoU=0.50:     {metrics_dict['AP_0.50']:.3f}\")\n",
    "print(f\"  Average Recall:     {metrics_dict['AR_100']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size analysis\n",
    "model_size_mb = os.path.getsize(MODEL_SAVE_PATH) / (1024 * 1024)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"  Model file size: {model_size_mb:.1f} MB\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Parameters (millions): {total_params/1e6:.2f}M\")\n",
    "\n",
    "# Memory usage analysis\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Run a forward pass to measure peak memory\n",
    "    dummy_input = torch.randn(1, 3, 300, 300).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    peak_memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "    print(f\"  Peak GPU memory: {peak_memory_mb:.0f} MB\")\n",
    "else:\n",
    "    print(\"  GPU memory analysis not available (CPU mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference speed benchmarking\n",
    "print(\"Running inference speed benchmark...\")\n",
    "\n",
    "# Create benchmark dataloader with larger batch size if possible\n",
    "benchmark_batch_size = 8 if DEVICE.type == 'cuda' else 4\n",
    "benchmark_dataloader = DataLoader(dataset, batch_size=benchmark_batch_size, \n",
    "                                shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Warmup runs\n",
    "print(\"Warming up GPU...\")\n",
    "with torch.no_grad():\n",
    "    for i, (images, _) in enumerate(benchmark_dataloader):\n",
    "        if i >= 5:  # 5 warmup batches\n",
    "            break\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        _ = model(images)\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Actual timing\n",
    "print(\"Running benchmark...\")\n",
    "total_images = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(benchmark_dataloader, desc=\"Benchmarking\"):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        \n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        _ = model(images)\n",
    "        \n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        total_images += len(images)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "fps = total_images / total_time\n",
    "\n",
    "print(f\"\\nInference Performance:\")\n",
    "print(f\"  Total images processed: {total_images}\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"  Average FPS: {fps:.2f}\")\n",
    "print(f\"  Average time per image: {1000/fps:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def visualize_predictions(model, dataset, num_samples=4):\n",
    "    \"\"\"Visualize model predictions on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get a sample\n",
    "            img, target = dataset[i * 100]  # Skip some samples for variety\n",
    "            \n",
    "            # Run inference\n",
    "            img_tensor = img.unsqueeze(0).to(DEVICE)\n",
    "            prediction = model(img_tensor)[0]\n",
    "            \n",
    "            # Convert image for visualization\n",
    "            img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "            img_np = (img_np * np.array([0.229, 0.224, 0.225]) + \n",
    "                     np.array([0.48235, 0.45882, 0.40784]))\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.imshow(img_np)\n",
    "            \n",
    "            # Filter predictions by confidence\n",
    "            conf_threshold = 0.3\n",
    "            high_conf_mask = prediction['scores'] > conf_threshold\n",
    "            \n",
    "            boxes = prediction['boxes'][high_conf_mask].cpu()\n",
    "            labels = prediction['labels'][high_conf_mask].cpu()\n",
    "            scores = prediction['scores'][high_conf_mask].cpu()\n",
    "            \n",
    "            # Draw bounding boxes\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                x1, y1, x2, y2 = box\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                \n",
    "                # Create rectangle\n",
    "                rect = patches.Rectangle((x1, y1), width, height, \n",
    "                                       linewidth=2, edgecolor='red', \n",
    "                                       facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add label\n",
    "                class_name = VOC_CLASSES[label]\n",
    "                ax.text(x1, y1-5, f'{class_name}: {score:.2f}', \n",
    "                       color='red', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "            \n",
    "            ax.set_title(f'Sample {i+1} - {len(boxes)} detections (conf > {conf_threshold})')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('SSD300 Predictions on Pascal VOC 2012', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Generating prediction visualizations...\")\n",
    "visualize_predictions(model, dataset, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Training loss curve\n",
    "ax1.plot(range(1, len(training_losses)+1), training_losses, 'b-o', linewidth=2, markersize=6)\n",
    "ax1.set_title('Training Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. mAP metrics\n",
    "map_scores = [metrics_dict['AP_0.50:0.95'], metrics_dict['AP_0.50'], metrics_dict['AP_0.75']]\n",
    "map_labels = ['mAP@0.5:0.95', 'mAP@0.5', 'mAP@0.75']\n",
    "bars = ax2.bar(map_labels, map_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "ax2.set_title('COCO mAP Scores', fontweight='bold')\n",
    "ax2.set_ylabel('Average Precision')\n",
    "ax2.set_ylim(0, max(map_scores) * 1.2)\n",
    "for bar, score in zip(bars, map_scores):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Top performing classes\n",
    "sorted_classes = sorted(per_class_ap.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "class_names, class_aps = zip(*sorted_classes)\n",
    "y_pos = np.arange(len(class_names))\n",
    "ax3.barh(y_pos, class_aps, color='lightsteelblue')\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(class_names)\n",
    "ax3.set_xlabel('Average Precision')\n",
    "ax3.set_title('Top 10 Classes by mAP', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Model statistics\n",
    "stats_data = {\n",
    "    'Model Size\\n(MB)': model_size_mb,\n",
    "    'Parameters\\n(Millions)': total_params/1e6,\n",
    "    'Inference Speed\\n(FPS)': fps,\n",
    "    'Peak Memory\\n(MB)': peak_memory_mb if DEVICE.type == 'cuda' else 0\n",
    "}\n",
    "\n",
    "stats_names = list(stats_data.keys())\n",
    "stats_values = list(stats_data.values())\n",
    "bars = ax4.bar(stats_names, stats_values, color=['gold', 'orange', 'lightgreen', 'pink'])\n",
    "ax4.set_title('Model Performance Statistics', fontweight='bold')\n",
    "ax4.set_ylabel('Value')\n",
    "for bar, value in zip(bars, stats_values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stats_values)*0.01, \n",
    "             f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('SSD300 Training and Evaluation Summary', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "summary_report = f\"\"\"\n",
    "==================================================\n",
    "           SSD300 TRAINING & EVALUATION SUMMARY\n",
    "==================================================\n",
    "\n",
    "DATASET:\n",
    "  • Pascal VOC 2012 ({len(dataset)} images)\n",
    "  • 20 object classes + background\n",
    "  • Converted to COCO format for evaluation\n",
    "\n",
    "MODEL ARCHITECTURE:\n",
    "  • SSD300 with VGG16 backbone\n",
    "  • Pre-trained on COCO, fine-tuned on VOC\n",
    "  • Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n",
    "  • Model file size: {model_size_mb:.1f} MB\n",
    "\n",
    "TRAINING:\n",
    "  • Epochs: {EPOCHS}\n",
    "  • Learning rate: {LEARNING_RATE}\n",
    "  • Optimizer: Adam\n",
    "  • Initial loss: {training_losses[0]:.4f}\n",
    "  • Final loss: {training_losses[-1]:.4f}\n",
    "  • Loss reduction: {((training_losses[0] - training_losses[-1]) / training_losses[0] * 100):.1f}%\n",
    "\n",
    "EVALUATION RESULTS:\n",
    "  • mAP @ IoU=0.50:0.95: {metrics_dict['AP_0.50:0.95']:.3f}\n",
    "  • mAP @ IoU=0.50:     {metrics_dict['AP_0.50']:.3f}\n",
    "  • mAP @ IoU=0.75:     {metrics_dict['AP_0.75']:.3f}\n",
    "  • Average Recall:     {metrics_dict['AR_100']:.3f}\n",
    "\n",
    "PERFORMANCE:\n",
    "  • Inference speed: {fps:.1f} FPS\n",
    "  • Time per image: {1000/fps:.1f} ms\"\"\"\n",
    "\n",
    "if DEVICE.type == 'cuda':\n",
    "    summary_report += f\"\"\"\n",
    "  • Peak GPU memory: {peak_memory_mb:.0f} MB\"\"\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "TOP PERFORMING CLASSES:\n",
    "\"\"\"\n",
    "\n",
    "for i, (class_name, ap) in enumerate(sorted_classes[:5]):\n",
    "    summary_report += f\"  {i+1}. {class_name}: {ap:.3f}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "FILES GENERATED:\n",
    "  • Model weights: {MODEL_SAVE_PATH}\n",
    "  • Predictions: {RESULTS_PATH}\n",
    "  • Metrics: {METRICS_PATH}\n",
    "  • COCO annotations: {COCO_JSON_PATH}\n",
    "\n",
    "==================================================\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary to file\n",
    "with open(\"outputs/training_summary.txt\", \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"Training and evaluation completed successfully!\")\n",
    "print(\"Summary saved to: outputs/training_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}